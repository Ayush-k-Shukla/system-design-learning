[
  {
    "file": "gaurav-sen\\intro.md",
    "content": "Parts\n\n[Part1](Part1.md)\n\n[Part2](Part2.md)\n"
  },
  {
    "file": "gaurav-sen\\Part1.md",
    "content": "# Part 1\r\n\r\n**Horizontal vs Vertical scaling**\r\nbuying more machine vs buy big machine\r\n![image.png](images/image.png)\r\n\r\n1. HLD (how our services going to communicate), LLD(How we are going to design classes)\r\n2. How to get started with a distributed system\r\n\r\n   1. vertical scaling: optimise precision and increase throughput with the same resources\r\n   2. preprossing (e.g cron job) : prepare before hand during non pick hours\r\n   3. Backups: keep backups and avoid single point of failure\r\n   4. horizontal scaling: get more resources\r\n   5. micro service architecture\r\n   6. distributed system (partioning)\r\n   7. load distribution\r\n   8. Decoupling\r\n   9. Logging\r\n   10. extensible\r\n\r\n3. Consistent Hashing [LINK](https://www.geeksforgeeks.org/consistent-hashing/)\r\n\r\n   1. Here the node(servers) and users(request) are put on the same ring, and whenever a request comes we traverse clockwise which ever server is found first that will serve the request.\r\n\r\n      1. The server nodes can be placed at random locations on this ring which can be done using hashing.\r\n         ![image.png](images/image3.png)\r\n\r\n      2. But suppose if node 2 fails then all the load of node 2 will be transefered to node 5 that will be an issue. so to solve this problem what we did we make the virtual servers\r\n         1. like the servers are placed on the ring by using one hash function but what if we use multiple (let say k) hash function and we can place same server at k different places. this will solve above problem.\r\n         2. the best solution is not to use K hash functions, but to generate K replica ids for each server id. Designing K hash functions while maintaining random uniformity and consistency is hard. Generating K replica ids is easy: xxx gives K replicas xxx + '1', xxx + '2', ..., xxx + 'K'. Then you take these replicas and generate K points on the ring with the same hash function\r\n\r\n4. Message/Task Queue [LINK](https://www.geeksforgeeks.org/message-queues-system-design/)\r\n\r\n   1. A message queues is a form of service-to-service communication that facilitates asynchronous communication. It asynchronously receives messages from producers and sends them to consumers.\r\n   2. Messaging Queues provide useful features such as persistence, routing, and task management.\r\n   3. Servers are processing jobs in parallel. A server can crash. The jobs running on the crashed server still needs to get processed.\r\n   4. A notifier constantly polls the status of each server and if a server crashes it takes ALL unfinished jobs (listed in some database) and distributes it to the rest of the servers. Because distribution uses a load balancer (with consistent hashing) duplicate processing will not occur as job_1 which might be processing on server_3 (alive) will land again on server_3, and so on.\r\n   5. This \"notifier with load balancing\" is a \"Message Queue\".\r\n   6. Message queue vs PUB sub [LINK2](https://medium.com/@osama94/pub-sub-system-vs-queues-9a5fd872f474), [LINK](https://systemdesignschool.io/blog/message-queue-vs-pub-sub),\r\n\r\n5. Micro services vs Monolith [LINK](https://www.geeksforgeeks.org/monolithic-vs-microservices-architecture/#what-is-a-monolithic-architecture)\r\n\r\n   1. Monolith doesn’t mean that service will run only on one machine, it can run on multiple\r\n\r\n      ![image.png](images/image%201.png)\r\n\r\n      points for when to choose which\r\n\r\n      ![image.png](images/image%202.png)\r\n\r\n## 6. DB Sharding [WORKING](https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6)\r\n\r\n1.  There are two type of partioning in DB Vertical AND Horizontal. [LINK](https://stackoverflow.com/questions/18302773/what-are-horizontal-and-vertical-partitions-in-database-and-what-is-the-differen)\r\n    1.  **Horizontal partitioning** involves putting different rows into different tables. Perhaps customers with ZIP codes less than 50000 are stored in CustomersEast, while customers with ZIP codes greater than or equal to 50000 are stored in CustomersWest. The two partition tables are then CustomersEast and CustomersWest, while a view with a union might be created over both of them to provide a complete view of all customers.\r\n    2.  **Vertical partitioning** involves creating tables with fewer columns and using additional tables to store the remaining columns. Normalization also involves this splitting of columns across tables, but vertical partitioning goes beyond that and partitions columns even when already normalized.\r\n2.  Sharding a database is a common scalability strategy for designing server-side systems. The server-side system architecture uses concepts like sharding to make systems more scalable, reliable, and performant.\r\n3.  Sharding is the horizontal partitioning of data according to a shard key. This shard key determines which database the entry to be persisted is sent to. Some common strategies for this are reverse proxies.\r\n4.  There are also some issue in sharding\r\n    1.  suppose when querying we need data from 2 shards then we have to perform join (kind of expensive and complex)\r\n    2.  Also as when we design DB we sharded so it has only fixed number of shard\r\n        1.  one solution to this is dynamically break existing shard into more sub shard\r\n5.  Best practice\r\n    1.  we can add indexes to each shard based on need\r\n"
  },
  {
    "file": "gaurav-sen\\Part2.md",
    "content": "# Part2\n\n1. Cache\n   1. Cache use case\n      1. avoid network calls (storing a profile of each user in an in-memory cache)\n      2. avoid repeated computation (find average ages of users) - we can precompute and store\n      3. Reduce DB calls\n   2. when we want to load and evict our data from the cache is called `cache-policy`. LRU(Least Recently Used) is the most common policy.\n   3. Cache invalidation: [LINK](https://www.geeksforgeeks.org/cache-invalidation-and-the-methods-to-invalidate-cache/#why-cache-invalidation-is-important)\n   4. Where to add cache\n      1. near to the server (in memory)\n         1. fast and simple to Implement\n         2. increases server memory size\n      2. Global cache\n         1. Like Redis so safe from server failures\n         2. higher accuracy\n         3. can scale independently\n   5. Distributed cache\n2. API design(things to consider)\n   1. Naming (based on what it does)\n   2. Define parameters (avoid passing parameters if not necessary)\n   3. Define Response (prefer objects as they are extensible)\n   4. Define Errors\n   5. Use GET/POST\n   6. Side effect (avoid API having side effects)\n      1. a side-effect is like you defined an API **setAdmin(groupId, admins);** if groupId is not present, you are creating a group that will be a side-effect.\n   7. Pagination and Fragmentation (when API response is huge)\n3. Event-driven services\n   1. Pub/Sub\n"
  },
  {
    "file": "premier\\Asynchronism.md",
    "content": "# **Asynchronism**\n\nThis means handling and processing task independently of the main workflow, allowing systems to be more scalable, responsive and efficient.\n\n[Message Queues](Scalability-files/Message-Queue.md)\n\n[Task Queues](Scalability-files/Task-Queue.md)\n\n## **Back Pressure**\n\n1. **Back pressure** is a mechanism used in **message queues, task queues, and network systems** to prevent **overloading** a system when it cannot process incoming requests fast enough. It helps maintain system stability by slowing down producers or rejecting new tasks when consumers (workers) are overwhelmed.\n\n### **Why Occurs**\n\n1. Producer sends tasks/messages faster than consumer can process.\n2. Workers are slow due to hight CPU/memory usage\n3. Queue storage is full\n\n### **Techniques to handle back pressure**\n\n| Technique                | How It Works                         | Use Case                           |\n| ------------------------ | ------------------------------------ | ---------------------------------- |\n| **Throttling**           | Limit request rate                   | Prevent API abuse, control traffic |\n| **Load Shedding**        | Drop low-priority tasks              | Logging, analytics, monitoring     |\n| **Auto Scaling**         | Add more workers when needed         | Cloud-based systems, Kubernetes    |\n| **Queue Acknowledgment** | Process tasks only when ready        | RabbitMQ, Kafka, Redis Streams     |\n| **Circuit Breaker**      | Stop requests to overloaded services | Microservices, APIs                |\n"
  },
  {
    "file": "premier\\attacks.md",
    "content": "# Attacks\n\n## Network based Attacks\n\n### Denial of a Service (DoS)\n\n- A DoS attack overwhelms a server, network, or system with excessive requests, making it unavailable to legitimate users. Unlike DDoS, DoS is typically performed from a single source.\n- **Scenario**\n\n  - Sending thousands of request per second to login page, causing resource exhaustion.\n  - exploiting a compute heavy API call to overload the backend.\n\n- **Prevention Strategies**\n\n  - **CAPTCHA:** Prevents automated scripts from bombarding your service\n  - **Rate Limiting:** Use tools like NGINX, Cloudflare, or AWS WAF to limit request rates per user/IP.\n  - **Throttling & Queuing:** Implement exponential backoff and queue requests.\n\n### Distributed Denial of a Service (DDoS)\n\n- A DDoS attack uses multiple compromised machines (botnets) to flood a system with traffic. Unline DoS it uses multiple compromised systems (forming botnets).\n- **Prevention**\n  - Use a CDN to offload traffic to edge servers\n  - Use Anycast Network Routing to ditribute traffic geographically.\n  - Use WAF(web app firewall) to filter traffic\n\n### Main-in-the-Middle (MitM)\n\n- An attacker intercepts communication between two parties, potentially modifying or stealing data.\n- **Scenarios**\n  - On public wifi intercept http traffic\n- **Prevention**\n  - Use HTTPS\n  - Encrypt sensitive data before transmission\n  - Add TLS and SSL certificates\n\n### DNS Spoofing\n\n- An attacker poisons a DNS resolver to redirect traffic to a malicious site.\n- **Scenarios**\n  - A user enters example.com, but due to a poisoned DNS cache, they are redirected to a fake example.com for phishing.\n- **Prevention**\n  - Use Secure Recursive DNS like (Google public DNS, Cloudfare DNS)\n  - Reduce TTL of DNS cache records\n\n## Web App Attacks\n\n### SQL Injection\n\n- SQL Injection occurs when an attacker manipulates a web application's SQL queries by injecting malicious SQL code. This can allow them to retrieve, modify, or delete database data.\n- **Example**\n\n  - User login Query\n\n    ```sh\n       SELECT * FROM users WHERE username = 'admin' AND password = 'password';\n    ```\n\n  - If an attacker inputs admin' -- as the username, the query becomes:\n\n    ```sh\n       SELECT * FROM users WHERE username = 'admin' -- ' AND password = 'password';\n    ```\n\n  - And will bypass authentication and give all data\n\n- **Prevention**\n\n  - Use Parametrized Queries which ensures that user input are treated as input no executable sql.\n    ```sh\n       await connection.execute(\n       \"SELECT \\* FROM users WHERE username = ? AND password = ?\",\n       [username, password]\n       );\n    ```\n  - Use ORMs as they will not allow direct SQL manipulation\n  - Validate and Sanitize user input\n  - Never use the root user for database queries. Create a dedicated user with minimum required permissions\n  - Dont expose DB error to client\n\n### Cross Site Scripting (XSS)\n\n- It is a client side attack where an attacker injects malicious scripts into web pages(usually js).\n- **Types**\n\n  - **Stored XSS**\n\n    - Malicious scipt is stored in the DB and it will execute everytime page load, affecting all users.\n    - **Example:** Attackers injects `<script>alert('hacked')</script>` into a comment field.\n\n  - **Reflected XSS**\n\n    - The malicious script is sent in a request (e.g., via URL or form input) but not stored on the server.\n    - **Example:**\n      - Attacker sends a phishing link `https://example.com/search?q=<script>stealCookies()</script>`\n      - If website directly reflect the param input without sanitization, the script executes in the victim's browser.\n\n  - **DOM based XSS**\n\n    - Happens entirely on the client-side (browser) by manipulating the DOM\n    - **Example:**\n    - `document.getElementById(\"output\").innerHTML = location.search;`\n    - The script will execute because `innerHTML` is used unsafely.\n\n  - **Prevention**\n\n    - **Escape User Input (Encode output)**\n\n      - Convert special characters into safe equivalents `(<, >, &, etc.)`.\n      - Instead of directly rendering use escaped version.\n\n        ```\n            function escapeHTML(str) {\n              return str.replace(/&/g, \"&&\")\n                    .replace(//g, \">>\")\n                    .replace(/\"/g, \"\"\"\")\n                    .replace(/'/g, \"''\");\n            }\n\n            use : <div>Welcome, {{ escapeHTML(username) }}</div>\n            insteadof: <div>Welcome, {{username}}</div>\n        ```\n\n      - React, Vue, and Angular escape HTML by default. But dangerous APIs like innerHTML still allow XSS.\n\n    - **Use Content Security Policy(CSP)**\n\n      - CSP blocks execution of untrusted scripts even if injected.\n      - Blocks inline scripts and external scripts unless explicitly allowed.\n      - set csp header to default-src 'self'\n\n    - **Use Secure JS functions**\n\n      - Avoid `innerHTML`, `document.write`, `eval`\n      - Instead use `textContent` or `createElement`\n\n    - Use `httpOnly` flag for cookies to prevent JS access\n    - Use secure lib like `DOMPurify` for FE and `helmet` for BE\n\n### Cross Site Request Forgery (CSRF)\n\n- It is an attack where an unauthorized request is sent from a user who is authenticated in a web app.\n- CSRF exploits the trust that a web application has in the user's browser, leveraging session cookies and stored authentication tokens.\n- **Working**\n\n  - victims logs into a website(e.g. bank) and receives an authentication cookie.\n  - now somehow attacker tricks victim to goto `evil.com` and from there attacker makes a reques to `bank` api.\n  - and as user is already authenticated so browser will automatically embed the auth cookies.\n  - The server processes the request because it trusts the authenticated user's cookies.\n\n- **Prevention**\n\n  - Just to mention CORS shows error on browser but still send request to server so will not help.\n  - **CSRF tokens**\n    - It is a random unique value generated by server and embedded into forms or headers.\n    - CSRF token are sent in form or headers as browser will no auto embed in every request.\n    - When client make the request it has to send csrf token in request. (localstorage store it)\n  - **Use `SameSite` Cookie Attriute**\n    - By setting cookies as `SameSite=strict` or `SameSite=lax`, browsers will prevent cookies from being sent with cross-site requests.\n  - Verify `Origin` and `Referrer` Headers\n    - limitation as some browser(like mobile apps) may not always include these headers\n    - Origin Header → Should match the site's domain.\n    - Referer Header → Should match the current URL.\n  - For Critical Actions (money transfer) ask user to reauth or MFA\n\n### Remote Code Execution\n\n- RCE happens when an attacker executes arbitrary code on the server due to improper input handling.\n- **Example**\n\n  ```sh\n    app.get('/exec', (req, res) => {\n      exec(req.query.command, (err, stdout) => {\n        res.send(stdout);\n      });\n    });\n  ```\n\n  - If an attacker sends:\n\n  ```\n    http://example.com/exec?command=rm -rf /\n  ```\n\n  - This could delete files on the server.\n\n- **Prevention**\n\n  - Avoid using `exec` and `eval`\n  - Run service with least previlages\n\n### Path Traversal Attack\n\n- Path Traversal exploits directory traversal vulnerabilities to access files outside the intended directory.\n- **Example**\n\n  ```sh\n    app.get('/download', (req, res) => {\n      const filePath = __dirname + \"/uploads/\" + req.query.file;\n      res.sendFile(filePath);\n    });\n  ```\n\n  - If an attacker sends:\n\n  ```\n    http://example.com/download?file=../../etc/passwd\n  ```\n\n  - If not properly handled, this reveals sensitive files.\n\n- **Prevention**\n\n  - Use Absolute Paths & Restrict Directory Access\n\n    ```sh\n\n      app.get('/download', (req, res) => {\n        const safePath = path.join(\\_\\_dirname, 'uploads', path.basename(req.query.file));\n        res.sendFile(safePath);\n      });\n    ```\n\n  - Only allow specific files for download\n\n## Authentication and Authorization Attacks\n\n### Brute Force Attack\n\n- It involves attackers using a dictionary or common used pwd or leaked creds and use authomated script to call.\n- **Prevention**\n  - Rate limiting\n  - Captcha\n  - Temporarily lock account after multiple failed attempts\n  - MFA\n\n### Session Hijacking\n\n- Session hijacking occurs when an attacker steals a valid session ID to impersonate a legitimate user. This allows them to access the system as the victim without knowing their password.\n- **Prevention**\n  - Use secure and HttpOnly Cookies\n  - Implement session expiry\n\n### OAuth and JWT Attacks\n\n- There can multiple way\n  - Stolen JWTs token can be used for prolonged access.\n  - Trick users into authorizing malicious apps.\n  - If the secret key is weak or exposed, attackers can forge JWTs.\n- **Prevention**\n  - Use strong JWT signing Algo\n  - Implement token expiry and rotation\n  - Allow only whitelisted callback URLs for OAuth redirect Urls\n  - Detect and remove compromised tokens (maintain blacklist in redis)\n"
  },
  {
    "file": "premier\\Availability-Consistency.md",
    "content": "# Availability and Consistency\n\n## Availability\n\n```jsx\nAvailability = Uptime / (Uptime + Downtime);\n```\n\n1. Uptime -The period during which system is functional and accessible\n2. Downtime - The period during which a system is unavailable due to failures, maintenance or other issues\n3. Availability is generally referred in terms of “nine”\n\n   ![https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe271d918-4a3b-4c74-ada6-6ce1b01a30ef_1632x912.png](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe271d918-4a3b-4c74-ada6-6ce1b01a30ef_1632x912.png)\n\n4. **Parallel Availability**\n\n   1. in parallel setup multiple component (or systems) operate independently. The systems overall availability is higher because it depends on the availability of at least one functioning component.\n   2. If one component fails, other components can still handle the request.\n\n      ```jsx\n      Availability (Total) = 1 - (1 - Availability (Foo)) * (1 - Availability (Bar))\n      ```\n\n5. **Sequential Availability**\n\n   1. In a sequence setup, multiple components depend on each other. The overall system is only available if all components in the sequence are operational.\n   2. A failure in any single component causes the entire system to fail.\n\n      ```jsx\n      Availability (Total) = Availability (Foo) * Availability (Bar)\n      ```\n\n6. **Strategies for improving availability**\n\n   1. Redundancy\n   2. Load balancing\n   3. Failover mechanism\n      1. It is the process of automatically switching to a backup system, component or resource when a primary system fails.\n      2. **Types**\n         1. **Active-Passive Failover**\n            1. The backup (passive) system is kept in a standby mode and becomes active only when the primary system fails.\n            2. e.g. A secondary database or server that takes over when the primary one crashes.\n         2. **Active-Active Failover**\n            1. Multiple systems (or nodes) are active and share the load simultaneously. If one fails, others continue handling the traffic.\n            2. e.g. Load-balanced web servers where all servers are active and share requests.\n   4. Data replication\n   5. Monitoring and Alerts\n\n## CAP Theorem\n\n1. CAP stands for **Consistency**, **Availability**, and **Partition Tolerance**, and the theorem states that:\n\n> **It is impossible for a distributed data store to simultaneously provide all three guarantees.**\n\n1. **Consistency (C)**\n\n   1. every read receives the most recent write or an error\n   2. This means that all working nodes in a distributed system will return the same data at any given time.\n   3. It is crucial for applications where having most up to date data is critical such as financial systems.\n\n2. **Availability (A)**\n\n   1. Every request (read or write) receives a non-error response, without the guarantee that it contains the most recent write.\n   2. This means the system is always operational and responsive even if response from some node don’t represent most up to date data.\n   3. It is important for system which need to remain operational all the time such as online retail system.\n\n3. **Partition Tolerance (P)**\n\n   1. The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.\n   2. Partition Tolerance means that the **system continues to function despite network partitions** where nodes cannot communicate with each other.\n   3. Partition Tolerance is essential for distributed systems because network failures can and do happen. A system that tolerates partitions can maintain operations across different network segments.\n\n4. **Trade-Off**\n\n   1. **CP**\n      1. These system prioritize consistency and can tolerate network partitions at cost of availability. During partition system may reject some requests to maintain consistency.\n      2. Traditional relational databases, such as MySQL and PostgreSQL, when configured for strong consistency, prioritize consistency over availability during network partitions.\n      3. Banking systems prefers consistency over availability\n   2. **AP**\n      1. These systems ensure availability and can tolerate network partitions, but at the cost of consistency. During a partition, different nodes may return different values for the same data.\n      2. like Cassandra and DynamoDB\n      3. Amazon’s Shopping cart designed to accept items always\n   3. **CA**\n      1. In the absence of partitions, a system can be both consistent and available. However, network partitions are inevitable in distributed systems, making this combination impractical.\n      2. Like single node DB, this is theoretically impossible\n   4. **In real world we use more flexible approaches like**\n      1. **Eventual consistency**\n         1. For many systems, strict consistency isn't always necessary.\n         2. Eventual consistency can provide a good balance where updates are propagated to all nodes eventually, but not immediately.\n         3. like DNS and CDN\n      2. **Strong Consistency**\n         1. A model ensuring that once a write is confirmed, any subsequent reads will return that value.\n      3. **Tunable Consistency**\n         1. It is allows developers to configure consistency based on specific needs.\n         2. Provide balance between strong and eventual consistency\n         3. Systems like Cassandra allow configuring the level of consistency on a per-query basis, providing flexibility.\n         4. Applications needing different consistency levels for different operations, such as e-commerce platforms where order processing requires strong consistency but product recommendations can tolerate eventual consistency.\n\n## **PACELC theorem**\n\n1. If a partition occurs (P), a system must choose between Availability (A) and Consistency (C). Else (E), when there is no partition, the system must choose between Latency (L) and Consistency (C).\n"
  },
  {
    "file": "premier\\Communication.md",
    "content": "# **Communication**\r\n\r\n![Image](./images/communication.jpg)\r\n\r\n## **HTTP**\r\n\r\n1. HTTP is a method for encoding and transporting data between a client and a server. It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request.\r\n2. HTTP is an application layer protocol relying on lower-level protocols such as **TCP** and **UDP**.\r\n\r\n| Verb   | Description                                               | Idempotent\\* | Safe | Cacheable                               |\r\n| ------ | --------------------------------------------------------- | ------------ | ---- | --------------------------------------- |\r\n| GET    | Reads a resource                                          | Yes          | Yes  | Yes                                     |\r\n| POST   | Creates a resource or trigger a process that handles data | No           | No   | Yes if response contains freshness info |\r\n| PUT    | Creates or replace a resource                             | Yes          | No   | No                                      |\r\n| PATCH  | Partially updates a resource                              | No           | No   | Yes if response contains freshness info |\r\n| DELETE | Deletes a resource                                        | Yes          | No   | No                                      |\r\n\r\n\\*Can be called many times without different outcomes.\r\n\r\n# HTTP/1.1 vs HTTP/2 vs HTTP/3\r\n\r\n| Feature              | HTTP/1.1                           | HTTP/2                                       | HTTP/3 (QUIC)                              |\r\n| -------------------- | ---------------------------------- | -------------------------------------------- | ------------------------------------------ |\r\n| **Transport**        | TCP                                | TCP                                          | **QUIC (UDP)**                             |\r\n| **Multiplexing**     | ❌ No (One request per connection) | ✅ Yes (Multiple requests in one connection) | ✅ Yes (No TCP limitations)                |\r\n| **HOL Blocking**     | ❌ Yes (Request-level)             | ❌ Yes (TCP-level)                           | ✅ No (QUIC eliminates it)                 |\r\n| **Connection Setup** | Slow (Multiple round trips)        | Faster                                       | **Fastest (0-RTT)**                        |\r\n| **TLS**              | Optional                           | Optional                                     | **Mandatory (TLS 1.3)**                    |\r\n| **Ideal for**        | Legacy systems, basic websites     | Modern web apps & APIs                       | Low-latency apps (Streaming, Gaming, VoIP) |\r\n| **Adoption**         | Still supported, but outdated      | Standard for web apps                        | Growing (Google, Cloudflare, Facebook)     |\r\n\r\n## Transmission control protocol (TCP)\r\n\r\n1. It is a connection oriented protocol. Connection is established and terminated using a handshake.\r\n2. All packets are guaranteed to reach destination in original order by\r\n   1. Checksum\r\n   2. Acknowledgement\r\n3. If sender does not get correct response then it will resend packets\r\n4. All these guarantees causes delays in TCP communication\r\n   Use TCP over UDP when:\r\n   1. You need all of the data to arrive intact\r\n   2. You want to automatically make a best estimate use of the network throughput\r\n\r\n## User datagram protocol (UDP)\r\n\r\n1. UDP is connectionless. Datagrams (analogous to packets) are guaranteed only at the datagram level. Datagrams might reach their destination out of order or not at all. UDP does not support congestion control. Without the guarantees that TCP support, UDP is generally more efficient.\r\n2. UDP is less reliable but works well in real time use cases such as VoIP, video chat, streaming, and realtime multiplayer games.\r\n3. Use UDP over TCP when:\r\n   1. You need the lowest latency\r\n   2. Late data is worse than loss of data\r\n   3. You want to implement your own error correction\r\n\r\n## Remote Procedure Call (RPC)\r\n\r\n1. RPC focuses on calling functions (procedures) on a remote server, like calling a local function in code. It is more action-oriented than REST.\r\n2. RPC abstracts the complexity of the communication process, allowing developers to focus on the logic of the procedure.\r\n3. **Key Characteristics**\r\n   1. **Action oriented**\r\n   2. **Function calls** - It allows you to call functions or method directly on a remote server. The client sends a reuest to execute a specific method and server returns the result.\r\n   3. **Synchronous Communication** - Client wait for the server to complete the called procedure.\r\n   4. **Variety of Protocols** - RPC can be implemented using different protocols, such as JSON-RPC, XML-RPC, or gRPC.\r\n   5. **Interface Definition Language** - RPC system often used an IDL to define the interface between client and server.\r\n4. **Advantages**\r\n   1. High Performance\r\n   2. Simpler to implement when need to perform specific actions\r\n   3. More flexible in handeling complex which do not fit CRUD model\r\n5. **Disadvantages**\r\n   1. Tight coupling of client and server\r\n   2. Less standardization\r\n   3. Limited Tooling\r\n\r\n## Representational state transfer (REST)\r\n\r\n1. It is an architectural style for designing networked applications.\r\n2. **Key Characteristics**\r\n   1. **Stateless**\r\n   2. **Resource oriented** - REST treats everything as a resource, such as users, products, or orders. Each resource is identified by a unique URL.\r\n   3. Leverages Standard HTTP methods\r\n   4. Follow client server architecture\r\n   5. **Cacheable** - REST response can be cached\r\n3. **Advantages**\r\n   1. **Scalability**\r\n   2. **Flexibility** - Allows different data formats likes JSON, XML or plaintext\r\n   3. **Standardization** - follow a standarized approcah that makes them easy to understand\r\n   4. **Wide Adoption**\r\n4. **Disadvantages**\r\n   1. **Verbosity** - when dealing with complex objects or relationships can be verbose\r\n   2. **Over-fetching/Under-fetching**\r\n   3. Inefficient for real-time applications (websockets and gRPC are better)\r\n\r\n## Google Remote Procedure Call(gRPC)\r\n\r\n1. gRPC is a high-performance, open-source RPC framework that uses Protocol Buffers (Protobuf) for communication instead of JSON. It supports streaming and is widely used in microservices.\r\n2. **Advantages**\r\n   1. **Fast & Efficient** – Uses Protobuf (binary format) instead of JSON, reducing payload size.\r\n   2. **Streaming Support** – Supports real-time communication (unlike REST).\r\n   3. **Strong Typing** – Uses `.proto` files for schema definition\r\n   4. **Great for Microservices** – Optimized for inter-service communication\r\n3. **Disadvantages**\r\n   1. **Not Human-Readable** – Protobuf is binary, making debugging harder.\r\n   2. **More Complex Setup** – Requires defining .proto files and generating client/server code.\r\n   3. **Limited Browser Support** – Requires a gRPC-web proxy to work in web applications.\r\n\r\n## GraphQL\r\n\r\n1. GraphQL is a query language that allows clients to request exactly the data they need, avoiding over-fetching and under-fetching.\r\n2. **Advantage**\r\n   1. No over/under fetching\r\n   2. Strog typing\r\n   3. Single endpoint\r\n   4. flexible queries - can fetch multiple related resources in one request\r\n3. **Disadvantages**\r\n   1. Complex to learn anf more setup than REST\r\n   2. Caching is harder\r\n   3. Increased server load (as we fetch all data from DB , only we dont send it to client)\r\n\r\n## Comparison\r\n\r\n| Feature               | REST                   | RPC                           | GraphQL            | gRPC                                 |\r\n| --------------------- | ---------------------- | ----------------------------- | ------------------ | ------------------------------------ |\r\n| **Data Structure**    | Resource-based (nouns) | Function-based (verbs)        | Query-based        | Function-based                       |\r\n| **Communication**     | HTTP (JSON/XML)        | HTTP (JSON)                   | HTTP (JSON)        | HTTP/2 (Protobuf)                    |\r\n| **Performance**       | Medium                 | Medium                        | Medium-High        | High (Binary)                        |\r\n| **Over-fetching**     | Yes                    | Yes                           | No                 | No                                   |\r\n| **Under-fetching**    | Yes                    | Yes                           | No                 | No                                   |\r\n| **Streaming Support** | No                     | No                            | No                 | Yes                                  |\r\n| **Caching**           | Easy (HTTP Caching)    | Hard                          | Hard               | Hard                                 |\r\n| **Best Use Case**     | Web APIs, Public APIs  | Simple actions, Internal APIs | Client-driven APIs | Microservices, High-performance APIs |\r\n\r\n## Websockets\r\n\r\n1. It enables full-duplex, bidirectional communication between a client over a single TCP connection.\r\n\r\n### Working\r\n\r\n<p align=\"center\">\r\n   <img src=\"images/ws-working.jpg\">\r\n</p>\r\n\r\n1. **Handshake**\r\n   1. The client initiates connection request using HTTP GET with and `upgrade` header set to `websocket`\r\n   2. if server support ws and accepts the request, it responds with a special `101` statud code, indicating protocol will be changed to websocket.\r\n2. **Connection**\r\n   1. Once handshake complete, the ws connection is established, connection will remain open untill it is closed by either the client and server.\r\n3. **Data transfer**\r\n   1. Both the client and server now sends message in realtime. these messages are sent in small packets called `frames`, and carry minimal overhead compared to HTTP.\r\n4. **Closure**\r\n   1. closed by with a `close` frame.\r\n\r\n### Usecase\r\n\r\n1. Real time collaboration tools\r\n2. Real time chat apps\r\n3. Live notifications\r\n4. Multiplayer online game\r\n5. Live streaming\r\n\r\n### Challenges and Considerations\r\n\r\n1. Some Proxy server don't support WS, and some firewall can also block.\r\n2. There should be a `fallback` implemented in case if client or network don't support WS. fallback mechanism can be `long-polling`.\r\n3. Need to secure by authentication and secure ws connection(wss://).\r\n\r\n### Long Polling\r\n\r\n<p align=\"center\">\r\n   <img src=\"images/long-polling.png\">\r\n</p>\r\n\r\n### Polling\r\n\r\n<p align=\"center\">\r\n   <img src=\"images/polling.jpg\">\r\n</p>\r\n\r\n## Server Sent Events (SSE)\r\n\r\n1. It allows server to send un-directional messages/events to client over HTTP.\r\n2. SSE is a technology that provides asynchronous communication with event stream from server to the client over HTTP for web applications. The server can send un-directional messages/events to the client and can update the client asynchronously. Almost every browser is supporting the SSE except Internet Explorer.\r\n\r\n### Working\r\n\r\n<p align=\"center\">\r\n   <img src=\"images/sse.webp\">\r\n</p>\r\n\r\n1. The server-sent events streaming can be started by the client’s GET request to Server.\r\n   ```\r\n   GET /api/v1/live-scores\r\n   Accept: text/event-stream\r\n   Cache-Control: no-cache\r\n   Connection: keep-alive\r\n   ```\r\n2. `Accept: text/event-stream` indicates the client waiting for event stream from the server, `Cache-Control: no-cache` indicates that disabling the caching and `Connection: keep-alive` indicates the persistent connection. This request will give us an open connection which we are going to use to fetch updates. After the connection, the server can send messages when the events are ready to send by the server. The important thing is that events are text messages in `UTF-8` encoding.\r\n3. List of pre-defined SSE field names include:\r\n   1. **event:** the event type defined by application\r\n   2. **data:** the data field for the event or message.\r\n   3. **retry:** The browser attempts to reconnect to the resource after a defined time when the connection is lost or closed.\r\n   4. **id:** id for each event/message\r\n\r\n### Usecase\r\n\r\n1. E-commerce Projects (notify whenever the user needs the information)\r\n2. Tracking system\r\n3. Alarm/Alert Projects\r\n4. IoT Projects (Alarm, notify, events, rules, actions)\r\n5. Stock Markets (Bitcoin etc.)\r\n6. Breaking news, Sports Score Updates\r\n7. Delivery projects\r\n8. In-app notifications\r\n\r\n### Challenges and Considerations\r\n\r\n1. One potential downside of using Server-Sent Events is the limitations in data format. Since SSE is restricted to transporting UTF-8 messages, binary data is not supported.\r\n2. When not used over HTTP/2, another limitation is the restricted number of concurrent connections per browser. With only six concurrent open SSE connections allowed at any given time, opening multiple tabs with SSE connections can become a bottleneck.\r\n   1. In HTTP/1.1, each SSE connection requires a separate TCP connection because there is no built-in multiplexing. However, HTTP/2 solves this by allowing multiple independent streams over a single TCP connection.\r\n\r\n## WebSocket vs. SSE vs. Long Polling vs. Polling\r\n\r\n| Feature                   | WebSocket                                   | SSE (Server-Sent Events)            | Long Polling                            | Polling                        |\r\n| ------------------------- | ------------------------------------------- | ----------------------------------- | --------------------------------------- | ------------------------------ |\r\n| **Communication Type**    | Full-duplex (bidirectional)                 | Unidirectional (server to client)   | Half-duplex (client-initiated)          | Half-duplex (client-initiated) |\r\n| **Latency**               | Very low                                    | Low                                 | Medium                                  | High                           |\r\n| **Efficiency**            | High (single persistent connection)         | High (single persistent connection) | Medium (delayed responses)              | Low (frequent requests)        |\r\n| **Connection Overhead**   | Single handshake, then persistent           | Single HTTP connection (kept open)  | New request for each update             | New request for each update    |\r\n| **Server Load**           | Low                                         | Low (compared to polling)           | Higher than SSE/WebSockets              | Very high                      |\r\n| **Client Requests**       | One per session                             | One per session                     | Multiple but less frequent              | Frequent requests              |\r\n| **Multiplexing (HTTP/2)** | Yes                                         | Yes (multiple streams possible)     | No                                      | No                             |\r\n| **Best Use Cases**        | Real-time apps (chats, games, live updates) | Notifications, live data feeds      | Notifications, moderate real-time needs | Basic periodic updates         |\r\n| **Scalability**           | High                                        | High                                | Moderate                                | Low                            |\r\n"
  },
  {
    "file": "premier\\Databases.md",
    "content": "# **Databases**\n\n## Relational Database\n\n1. Collection of data items organized in tables\n2. **ACID Properties** fllowed by a Relational Database transaction (a transaction is a single unit of work or sometimes madeup of multiple transactions)\n   1. **Atomicity**\n      1. It ensures that a transaction is treated as a single unit.\n      2. It means either all steps of transaction complete or none of them.\n      3. If any part of transaction fails, the transaction is rolled back and the database is restored to the previous consistent state.\n      4. How DB handles Atomicity\n         1. Databases unses something called logs (write ahead log) which keeps tracks of all the changes made during an transaction.\n         2. Steps\n            1. Transaction started\n            2. Operations will be performed in memory\n            3. Transaction writes all the logs\n            4. If all steps are successful, the transaction will commit and make the changes permanent\n            5. if any fails then rollback will be performed using the Logs\n   2. **Consistency**\n      1. It means any transaction is moving databse from one consistent state to another consistent state.\n      2. It takes care of all the constraints (uniqueness, foreign keys) to be satisfied before and after the transaction.\n      3. If any rule fails transaction will not be committed.\n      4. **e.g.** Consider a database with constraint account balance can't be negative. A transaction attempting to withdraw more money than available will fail the check so the transaction should be rolled back.\n      5. **Consistency in Distributed Systems**\n         1. Strong Consistency\n            1. This guarantees that once a transaction is completed the latest data is available in all the nodes immediately.\n         2. Eventual Consistency\n            1. If a system prioritize availibility then data might take some time to be synchronize across multiple regions. Eventually all nodes will be consistent but in short term different nodes might see different version of data.\n   3. **Isolation**\n      1. It ensures that multiple transactions runs in isolation and dont effect other transaction.\n      2. Isolation prevents data inconsistencies that can arise when 2 transactions access and update same data.\n      3. Databases uses Locks (Shared, Exclusive) to ensure that Isolation.\n      4. **Isolation levels**\n         1. It comes with different isolation levels each having balance between performance and data integrity.\n         2. **Read Uncommited**\n            1. The lowest level where transactions can see uncomitted changes of each other\n            2. Data corruption is high as the final value depends on the order of commit. Last one wins\n            3. It is rarely used\n            4. e.g. Transaction A updates `balance` to 1000 but doesn't commit yet. Transaction B can read the `balance` as 1000 eventhough it is not permanent yet.\n         3. **Read Commited**\n            1. A transaction can only read committed changes.\n            2. It guarantees No **Dirty Reads** (when a transaction reads data that has been modified by another transaction, but not yet committed), and can be **Non Repeatable Read** (a transaction reads data, and then another transaction modifies or deletes that data before the first transaction complete.)\n            3. Used in applications where exact consistency is less critical and performance is priority.\n            4. e.g. TA reads `balance` as 1000 and continues and in between TB updates `balance` to 1002 so in second read TA will read inconsistent value.\n         4. **Repeatable Read**\n            1. Once a transaction reads data, It is locked until the transaction completes.\n            2. This guarantees no dirty reads, No Non Repeatable Reads. **Phantom reads** (occurs when two same queries are executed, but the rows retrieved by the two, are different, bcs new row is added or existing got deleted by some transaction) may occur.\n         5. **Serializable**\n            1. Highest level of isolation.\n            2. Transaction executed in a serializable manner, as if they run one after another.\n            3. This gives no Dirty reads, No Non Repeatable Reads, and no phantom reads.\n   4. **Durability**\n      1. Once a transaction is committed, its changes are permanent even in the case of a crash.\n      2. How it works\n         1. This is achieved generally by writing data to a non-volatile storage (SSD, Hard drives, cloud storage) once the transaction is commited.\n         2. Some things used here are WAL (write ahead log) and Checkpointing.\n3. **Master slave replication**\n   1. Only one master\n   2. Slave only serve reads, and master server both read and write.\n   3. Master replicates writes to slaves\n   4. If master fails then either the DB can operate in read mode or we have to promote any slave to master or provision a new master.\n   5. Disadvantages\n      1. potential loss if master fails before any newly written data is written to slaves.\n      2. Lot of write replays to slave can slow down slave.\n4. **Master Master replication**\n   1. Two or more servers act as master.\n   2. Both masters serve reads and writes and coordinate with each other on writes. If either master goes down, the system can continue to operate with both reads and writes.\n   3. Disadvantage\n      1. All of master-slave\n      2. need some logic to figure out where to write\n      3. conflict resolution is time consuming\n5. **Federation**\n   1. Federation (Functional Partitioning) splits up the database by function. for e.g. instead of a single monolith database you can have three database : forums, users and products, resulting in less read and write traffic to each DB.\n   2. Small database with same data can reduce cache miss as well.\n   3. Disadvantage\n      1. Not effective if our schema requires huge functions or tables\n      2. Need to update application logic to determine which database to read and write.\n      3. Joining data from two database is complex.\n6. **Sharding** [ref1](../gaurav-sen/Part1.md#7-db-sharding-working) [ref2](./Scalability-files/Database-Scaling.md)\n7. **SQL Tuning**\n   1. SQL tuning is the process of optimizing SQL queries and the database schema to improve the performance of a system.\n   2. Some common ways\n      1. Avoid expensive joins\n      2. Use Good indices\n      3. Partitioning, sharding in better way\n8. **Denormalization**\n   1. It helps to improve read performance at the cost of some write performance. Here some copies of data are duplicates across tables to avoid expensive joins.\n   2. In most system reads are used more than write to increasing read performance is good.\n\n## Non-Relational Database (NoSql)\n\n1. Here Data is denormalized, and joins are generally done in the application code.\n2. Most NoSql Databases lacks true ACID transactions and favours eventual consistency.\n3. For these BASE is used to describe properties in comparison to [CAP theorem](./Availability-Consistency.md#cap-theorem).\n   1. Basically available\n   2. Soft state\n   3. Eventual Consistency\n4. Types\n   1. **Key value stores**\n      1. It allows for O(1) reads.\n      2. It allows to store metadata with a value.\n      3. It commonly used in `Session storage` , `Caching` and `Real time data processing`.\n      4. e.g. Redis, DynamoDB.\n   2. **Document Store**\n      1. A document store is centered around documents (XML, JSON, binary, etc), where a document stores all information for a given object.\n      2. It provides APIs or a query language to query based on the internal structure of documents.\n      3. Documents are organized by collections, tags, metadata or directories.\n      4. Useful where data model evolves over time.\n      5. It is commonly used in `CMS` , `E-commerce`.\n      6. e.g. MongoDB, CouchDB.\n   3. **Graph Database**\n      1. They represent data as graphs, consisting of nodes (entities), edges (relationships between entities), and properties (information associated with nodes and edges).\n      2. Commonly useful in `Social Network`, `Knowledge Graphs`, `Recommendation Systems`.\n      3. e.g. Neo4j, Amazon Neptune.\n   4. **Wide Column**\n      1. A wide-column database is a type of NoSQL database in which the names and format of the columns can vary across rows, even within the same table. Wide-column databases are also known as column family databases.\n      2. A wide column store's basic unit of data is a column (name/value pair). A column can be grouped in column families (analogous to a SQL table). Super column families further group column families. You can access each column independently with a row key, and columns with the same row key form a row. Each value contains a timestamp for versioning and for conflict resolution.\n      3. They organize data into tables with a flexible and dynamic column structure. They are designed to handle large-scale, distributed data storage and provide high scalability and performance.\n      4. e.g. HBase, Google Bigtable, Apache Cassandra\n      5. ![Image](./images/wide-column-db-1.png) ![Image](./images/wide-column-db-2.png)\n\n## SQL vs NoSql\n\n![Image](./images/SQl-vs-Nosql.png)\n"
  },
  {
    "file": "premier\\DNS-CDN-Load_balancer-Proxies.md",
    "content": "# DNS, CDN, Load balancer, Proxies\n\n### Domain Name System\n\n1. It maps a domain name to IP address.\n\n![image.png](./images/image.png)\n![image.png](./images/how-does-dns-resolution-work.webp)\n\n2. **Steps**\n   1. The browser (client) checks if the hostname to IP address mapping exists in the local cache of the client.\n   2. If the last step failed, the client checks the Operating System (OS) local cache by executing a system call (syscall).\n   3. If the last step failed, the client makes a DNS request to the Gateway/Router and checks the local cache of the Router.\n   4. If the last step failed, the router forwards the request to Internet Service Provider (ISP) and checks the DNS cache of the ISP.\n   5. If the last step fails, the DNS resolver queries the root servers (there are 13 root servers with replicas worldwide).\n   6. DNS resolver queries Top Level Domain (TLD) servers such as .com, or .org. DNS resolver queries Authoritative name servers such as google.com.\n   7. Optionally, the DNS resolver queries Authoritative subdomain servers such as maps.google.com depending on your query.\n\n### CDN\n\n1. It is a globally distributed network of connected servers , serving data from location near to user.\n2. Generally it is used for serving static data like images, html, css, js but some CDN’s also support dynamic content.\n3. **Types**\n   1. **Pull**\n      1. It like you added a new post with static data to your app, and some different location person tries to access in that case first time the cdn will take time to load the data as it is not present there. so means when user access a data which is not on cdn it takes a pull and store it nearby.\n   2. **Push**\n      1. instead of waiting around for the CDN to pull the content when it’s needed, you simply upload the entire content of your travel blog to the CDN beforehand. That way your pictures, theme files, videos, and the rest are always on the CDN servers around the world.\n   3. Generally setting up a pull CDN is easy. Once it is initially configured a pull CDN seamlessly stores and updates content on it’s server as requested. The data generally stays there for 24 hours if not modified .\n      1. However, what makes a pull CDN easy to use can also be a drawback. When making changes to a blog, you typically don't have control over how long the pull CDN cache lasts. If you update an image, it might take up to 24 hours to reflect the changes, unless you shut off the CDN or clear its cache.\n   4. The decision on which CDN type to go with revolves in large part around traffic and downloads. Travel blogs that are hosting videos and podcasts (aka. large downloads) will find a push CDN cheaper and more efficient in the long run since the CDN won’t re-download content until you actively push it to the CDN. A pull CDN can help high-traffic-small-download sites by keeping the most popular content on CDN servers. Subsequent updates (or “pulls”) for content aren’t frequent enough to drive up costs past that of a push CDN.\n4. **Benefits**\n   1. The request fulfilled by your CDN will not go to server\n   2. less distance travelled means faster response time\n5. **Disadvantages**\n   1. Content might be stale if it is updated before the TTL expires it.\n   2. CDN costs could be significant depending on traffic, although this should be weighed with additional costs you would incur not using a CDN.\n\n### Load Balancer\n\n1. It distributes incoming client requests to computing resources such as application servers and databases.\n2. These are effective at\n   1. Preventing overloading resources\n   2. Preventing requests from going to unhealthy servers\n   3. Helping eliminate single point of failure\n3. **Types**\n   1. **Hardware Load balancer**\n      1. Physical devices designed for high-speed traffic distribution.\n      2. Expensive and used in enterprise setup\n      3. e.g. Citrix ADC\n   2. **Software Load Balancers**\n      1. Applications or services that run on standard servers\n      2. Cost effective and flexible\n      3. e.g. HAProxy, Nginx\n   3. **Cloud Load Balancers**\n      1. Managed service provided by cloud providers\n      2. Scalable and easy to integrate\n      3. e.g. Google Cloud LB, Amazon Elastic LB\n4. **Types based on functionalities**\n   1. **Layer 4 (Transport layer) LB**\n      1. Operates at the TCP/UDP layer.\n      2. Routes traffic based on IP addresses and ports.\n      3. Example: AWS Network Load Balancer (NLB).\n   2. **Layer 7 (Application layer) LB**\n      1. Operates at the HTTP/HTTPS layer.\n      2. Makes routing decisions based on URL, headers, cookies, etc.\n      3. Example: AWS Application Load Balancer (ALB).\n5. **Types of Load Balancing Algorithms**\n   1. **Round Robin**\n      1. A request is sent to the first server in the list.\n      2. The next request is sent to the second server, and so on.\n      3. After the last server in the list, the algorithm loops back to the first server.\n      4. Preferred only when all servers have same processing capabilities, will cause issue if some servers has diff processing capabilities.\n   2. **weighted round robin**\n      1. Each server is assigned a weight based on their processing power or available resources.\n      2. Servers with higher weights receive a proportionally larger share of incoming requests.\n      3. More complex to implement and does not consider server current load or response time.\n   3. **IP hashing**\n      1. Calculates a hash value from the client’s IP address and uses it to determine the server to route the request.\n      2. Used when we need session persistence, as request from same client are always directed to same server\n      3. Can lead to uneven load distribution if certain IP addresses generate more traffic than others.\n      4. Lacks flexibility if a server goes down, as the hash mapping may need to be reconfigured.\n   4. **Least connections**\n      1. Monitor the number of active connections on each server.\n      2. Assigns incoming requests to the server with the least number of active connections.\n   5. **Least response time**\n      1. Monitors the response time of each server\n      2. Assigns incoming requests to the server with the fastest response time\n      3. May not consider other factors such as server load or connection count.\n6. **Disadvantage**\n   1. A single load balancer is a single point of failure, configuring multiple load balancers further increases complexity.\n   2. The load balancer can become a performance bottleneck if it does not have enough resources or if it is not configured properly.\n\n## Proxy Servers\n\n![https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87424c89-0ba3-4580-89bb-ccd5870dff69_945x419.png](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87424c89-0ba3-4580-89bb-ccd5870dff69_945x419.png)\n\n1. **Proxy (forward proxy) Server**\n   1. In computer terms, a proxy (or a forward proxy) is a server that acts on behalf of clients on a network.\n   2. When you send a request, like opening a webpage, the proxy intercepts it, forwards it to the target server, and then relays the server’s response back to you.\n   3. **How a proxy server handles a request**\n      1. The user types a website URL into their browser. The request is intercepted by the proxy server instead of going directly to the website.\n      2. The proxy server examines the request to decide if it should forward it, deny it, or serve a cached copy.\n      3. If the proxy decides to forward the request, it contacts the target website. The website sees only the proxy server’s IP, not the user’s.\n      4. When the target website responds, the proxy receives the response and relays it to the user.\n   4. **Benefits**\n      1. Privacy and Anonymity\n         1. proxy server hide our IP address so the destination server don’t our real location\n      2. Access Control\n      3. Security\n         1. proxies can filter out malicious content and block suspicious sites\n      4. Improved Performance\n         1. It also caches frequently accessed content\n   5. **Real world applications**\n      1. Bypassing Geographic Restrictions\n         1. Like there is a server in US which give data specific to user in US but you also want same data from india, in that case you will call proxy server setup in US and it will call main server, which will give desired response.\n      2. Speed and Performance Optimization (Caching)\n         1. with use of caching for most accessed data. it also uses TTL to deal with stale data\n2. **Reverse Proxy**\n   1. A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public. Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client.\n   2. Think of a reverse proxy as a **gatekeeper**. Instead of hiding clients from the server, it hides servers from clients.\n   3. Allowing direct access to servers can pose security risk like DDoS attack.\n   4. **How handles request**\n      1. Client make request to reverse poxy server and this reverse proxy server then route it to appropriate backend server based on load balancing and availability\n      2. BE server process the request and sends the request back to reverse proxy\n      3. reverse proxy relays the response to client\n   5. **Benefits**\n      1. Enhanced security - reduce risk of attacks on BE servers as not exposed\n      2. Load Balancing\n      3. Caching static content\n      4. **SSL termination**\n         1. Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations\n         2. Removes the need to install [X.509 certificates](https://en.wikipedia.org/wiki/X.509) on each server\n      5. **Web application Firewall (WAF)**\n         1. it can inspect incoming requests, acting as a firewall to detect and block malicious traffic\n   6. **Real world application**\n      1. **Cloudflare’s** reverse proxy is widely used by global websites and applications to boost speed, security, and reliability.\n      2. It’s **Web Application Firewall (WAF)** and **DDoS protection** blocks malicious traffic before it reaches the site’s servers, safeguarding against attacks and improving uptime.\n   7. **Disadvantage**\n      1. complexities\n      2. A single reverse proxy can introduce single point of failure\n3. **Some clarification**\n   1. VPN and proxy are not same as VPN encrypts all out internet traffic, and proxy only forwards specific request\n\n### Load Balancer vs Reverse Proxy\n\n1. Deploying a load balancer is useful when you have multiple servers. Often, load balancers route traffic to a set of servers serving the same function.\n2. Reverse proxies can be useful even with just one web server or application server, opening up the benefits described in the previous section.\n\nPending things\n\n1. Nginx architecture\n2. HAProxy architecture\n"
  },
  {
    "file": "premier\\High-level-tradeoffs.md",
    "content": "# High level Tadeoffs\n\n## **Performance vs Scalability**\n\n1. If you will have performance issue your system will be slow for a single user\n2. if you will have scalability issue your system will be fast for a user but will be slow under heavy load\n3. **Performance** refers to how efficiently a system executes tasks under a specific workload. It focuses on response time, throughput and resource usage\n4. **Scalability** refers ability of a system to not degrade the performance under increased workload or growing user.\n\n## **Latency vs Throughput**\n\n1. **Latency** is the time it takes a single request to be processed or a task to be completed. typically measured in milliseconds or seconds.\n2. **Throughput** refers to the rate at which a system process requests or tasks. typically measured in operations per second (ops), transactions per second.\n3. **Generally we thrive for maximal throughput with acceptable latency**\n\n## **Availability vs consistency - see in page [Availability and Consistency](Availability-Consistency.md)**\n\n## **Batch vs Stream Processing**\n\n| Feature             | Batch Processing                                                                 | Stream Processing                                                                       |\n| ------------------- | -------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |\n| **Data Processing** | Processes a large volume of data at once.                                        | Processes data as it arrives, record by record.                                         |\n| **Latency**         | High latency, as processing happens after data collection.                       | Low latency, providing near real-time insights.                                         |\n| **Throughput**      | Can handle vast amounts of data at once.                                         | Optimized for real-time but might handle less data volume at a given time.              |\n| **Use Case**        | Ideal for historical analysis or large-scale data transformations.               | Best for real-time analytics, monitoring, and alerts.                                   |\n| **Complexity**      | Relatively simpler to implement with predefined datasets.                        | More complex, requires handling continuous streams.                                     |\n| **Data Scope**      | Operates on a finite set of data.                                                | Operates on potentially infinite streams of data.                                       |\n| **Error Handling**  | Errors can be identified and corrected before execution.                         | Requires real-time handling of errors and failures.                                     |\n| **Resource Usage**  | Resource-intensive during processing, idle otherwise.                            | Continuous use of resources.                                                            |\n| **Cost**            | Cost-effective for large volumes of data.                                        | More expensive due to continuous processing.                                            |\n| **Tools**           | Apache Hadoop, Apache Spark (Batch Mode), AWS Glue, Google Dataflow (Batch Mode) | Apache Kafka, Apache Flink, Apache Storm, AWS Kinesis, Google Dataflow (Streaming Mode) |\n\n## Stateful vs Stateless Design\n\n<p align=\"center\">\n   <img src=\"images/stateless-vs-full.png\">\n</p>\n\n### Stateless\n\n1. Each request is treated as an independent operation\n2. Server doesn't store any information about the client's state between requests.\n3. **Advantages**\n   1. Scalability - can add multiple server, noneed to maintain session\n   2. Simplicity\n   3. Resilience - failure on one server won't disrupt user session.\n4. **Disadv**\n   1. Less context to server about client so personalization is not possible\n   2. Every request need some extra data to carry, leading to larger payloads\n5. **Examples**\n   1. REST APIs\n   2. Miscroservices\n   3. CDNs\n\n### Stateful\n\n1. System remembers client data from one request to next.\n2. It maintains a record of the client's state, which can include session information, transaction details, or any other data relevant to the ongoing interaction.\n3. **Advantages**\n\n   1. Personalized experience\n   2. Contextual continuity\n   3. Reduced payload\n\n4. **Disadv**\n\n   1. Scalability\n   2. Failure prone\n   3. Complex\n\n5. **Examples**\n\n   1. REST APIs with session storage\n   2. In Banking, E-commerce (cart), Multiplayer games\n\n## Concurrency vs Parallelism\n\nConcurrency is about managing multiple tasks simultaneously, while Parallelism is about executing multiple tasks at the same time.\n\n### Concurrency\n\n<p align=\"center\">\n   <img src=\"./images/concurrency.png\">\n</p>\n\n1. Concurrency means an application is making progress on more than one task at the same time.\n2. Even on a single CPU core. This is achieved through context switching, where the CPU rapidly switches between tasks, giving the illusion of simultaneous execution.\n3. This seamless switching—enabled by modern CPU designs—creates the illusion of multitasking and gives the appearance of tasks running in parallel.\n4. The primary objective of concurrency is to maximize CPU utilization by minimizing idle time.\n5. Cost : Although Context switching enables concurrency but it also introduce overhead\n   1. every switch require saving and restoring task states\n6. **Examples**\n   1. Modern web browsers perform multiple task concurrently (like loading html/css, responding user clicks etc.)\n   2. Used in web servers handling multiple client requests, even on a single CPU.\n   3. Chat apps\n\n### Parallelism\n\n<p align=\"center\">\n   <img src=\"./images/parallelism.png\">\n</p>\n<p align=\"center\">\n   <img src=\"./images/parallelism2.png\">\n</p>\n\n1. Multiple tasks are executed simultaneously.\n2. To achieve parallelism, an application divides its tasks into smaller, independent subtasks. These subtasks are distributed across multiple CPUs, CPU cores, GPU cores, or similar processing units, allowing them to be processed in parallel.\n3. Modern CPUs consist of multiple cores. Each core can independently execute a task. Parallelism divides a problem into smaller parts and assigns each part to a separate core for simultaneous processing.\n4. **Examples**\n   1. ML training - train modesls by dividing datasets into smaller batches\n   2. Web crawlers - breaks list of URls into smaller chunks and process in parallel\n   3. Data processing\n\n| **Aspect**               | **Concurrency**                                                                             | **Parallelism**                                                              |\n| ------------------------ | ------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |\n| **Execution**            | Achieved through context switching on a single core or thread.                              | Requires multiple cores or processors to execute tasks simultaneously.       |\n| **Focus**                | Managing multiple tasks and maximizing resource utilization.                                | Splitting a single task into smaller sub-tasks for simultaneous execution.   |\n| **Use Case**             | Best suited for I/O-bound tasks like handling multiple network requests or file operations. | Ideal for CPU-bound tasks like data processing or machine learning training. |\n| **Resource Requirement** | Can be implemented on a single core or thread.                                              | Requires multiple cores or threads.                                          |\n| **Outcome**              | Improves responsiveness by efficiently managing task switching.                             | Reduces overall execution time by performing tasks simultaneously.           |\n| **Examples**             | Asynchronous APIs, chat applications, or web servers handling multiple requests.            | Video rendering, machine learning training, or scientific simulations.       |\n| **Analogy**              | A single chef multitasking—preparing multiple dishes by working on them in parts.           | Multiple chefs working on different dishes at the same time.                 |\n"
  },
  {
    "file": "premier\\Scalability-files\\Caches.md",
    "content": "# Caches\n\n1. It is technique of temporarily store copies of data in high speed storage layers to reduce access time.\n2. **Benefits**\n\n   1. Performance Improvement\n   2. Improved Scalability\n   3. Reduced Load\n\n3. **Types**\n\n   1. **In-memory**\n      1. store data in main memory for fast access (Redis, Memcached)\n      2. mainly used for session management, store frequently accessed data and as a front for DB.\n   2. **Distributed**\n      1. Cache is shared in multiple nodes (Redis cluster, Amazon Elasticache)\n      2. It used for\n         1. Shared session data in microservices.\n         2. Global state management for large systems\n   3. **Client side**\n      1. Data is cached on client side (browser, mobile app) mainly in the form of cookies, local storage or app specific cache\n      2. Used\n         1. web browser to cache static assets to decrease load time\n         2. Offline support for mobile apps\n   4. **Database**\n      1. involves storing frequently queried DB result in a cache\n      2. Types\n         1. Query Cache - cache query result\n         2. Index Cache - keep index in-memory\n         3. Row Cache - ideal for complex and less changing queries\n   5. **CDN**\n      1. a mechanism where static and dynamic content is cached at geographically distributed servers (edge locations) closer to end-users.\n      2. It reduces latency, improve page load time\n\n4. **Caching strategies**\n\n   1. **Read through**\n\n      1. **Working**\n         1. The cache act as a transparent proxy between app and data store.\n         2. The app always interacts with cache\n         3. on cache miss the cache itself fetches the data from the data store, updates the cache, and returns the data to application.\n      2. **pros**\n         1. Simple, cache itself handle misses and populates itself\n         2. it ensures data is always fetched from single source(cache)\n      3. **cons**\n         1. increased complexity in cache logic\n         2. cache layer should support read through logic, which may require specific solutions\n      4. **use case**\n         1. when a cache system need to be abstracted away from app\n\n   2. **Write through**\n\n      1. Every write to DB is also written to the cache simultaneously\n      2. Ensures consistency but impacting write performance\n      3. **Pros**\n         1. ensures cache consistency with DB\n         2. No stale data\n      4. **Cons**\n         1. increased write latency since cache and DB updates are synchronous\n      5. **use case**\n         1. systems requiring strong consistency (banking app)\n\n   3. **Write back (Write behind)**\n\n      1. Data is written to cache first and later to synchronized with the database, improving write performance but risking data loss.\n      2. **use case**\n         1. write heavy app with tolerance for eventual consistency\n\n   4. **Cache aside (Lazy loading)**\n\n      1. **works**\n         1. The application explicitly interact with cache\n         2. if cache miss then application retrieves data from DB updated the cache, and then serves the response\n      2. **Pros**\n         1. Simple\n         2. Cache only contains requested data (reducing memory usage)\n      3. **cons**\n         1. initial cache miss cost lost of overhead\n      4. **use case**\n         1. frequently read with less update (e.g. product catalog)\n\n   5. **Write Around**\n\n      1. writes goes directly to the database, the cache is updated only when data is read later.\n      2. **Pros**\n         1. Reduced cache overhead\n      3. **cons**\n         1. Stale data risk\n         2. Cold cache - new or updated data is not immediately available in cache\n         3. not ideal of read heavy app\n      4. **use case**\n         1. Write heavy systems\n\n      ![https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d4f861a-82b2-4b8f-b9c7-d926f079a108_2163x3153.jpeg](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d4f861a-82b2-4b8f-b9c7-d926f079a108_2163x3153.jpeg)\n\n5. **Cache Eviction Policies**\n\n   1. To Manage limited size of cache, eviction policies are used to determine which data should be remove when cache is full\n   2. **LRU (Least recently)**\n      1. Removes the least recently accessed item first\n      2. Assumes that recently accessed data will likely to be accessed again soon\n      3. Often implemented using Doubly linked list\n      4. Used mostly when data access patterns exhibit temporal locality (The core is likely to access the *same* location again in the near future.)\n   3. **LFU (Least Frequently)**\n      1. Removes the least frequently accessed items first\n      2. Assume that frequently accessed data is more valuable and should remain in cache\n      3. Implemented by maintaining a count of access frequencies for each item.\n      4. Ideal for the scenarios with clear, repeated patterns (e.g. recommendation systems)\n   4. **FIFO**\n      1. removes the oldest data first regardless of how often or recently data is accessed.\n      2. Simple and deterministic\n      3. Implemented using queue\n      4. Used when freshness of data is more important\n   5. **TTL**\n      1. It is a time based eviction policy where data is removed from the cache after a specified duration\n      2. Ensures data freshness and consistency\n      3. Suitable for caches that deal with time sensitive data (e.g. stock prices, weather forecasts)\n\n6. **Challenges and considerations**\n\n   1. **Cache Coherence**\n      1. ensures data in cache remains consistent with source of truth(DB)\n      2. Solution\n         1. Implement TTL to periodically refresh\n   2. **Cache Invalidation**\n      1. Determining when and how to update and remove stale data from cache\n   3. **Cold start**\n      1. Handling scenarios like when the cache is empty such as during restart.\n      2. Solution\n         1. Prepopulate data with critical data\n         2. use cache aside pattern\n   4. **Cache Eviction policy**\n      1. Choose correct eviction policy\n      2. Analyze workload patterns to choose the correct policy\n      3. Monitor eviction metrices\n   5. **Cache penetration**\n      1. Preventing malicious attempts to repeatedly query for non-existent data, potentially overwhelming the backend.\n   6. **Cache stampede**\n      1. When many clients request the same data simultaneously, causing a cache miss and overwhelming the backend data source.\n      2. Solution\n         1. use locking to allow only one request data while others wait\n"
  },
  {
    "file": "premier\\Scalability-files\\Clone.md",
    "content": "# Clone\n\n1. Public servers of a scalable web service are hidden behind a load balancer.\n2. Sessions must be stored in a centralized data store accessible to all your application servers. It can be an external database or persistent cache, like Redis.\n3. To ensure that the same updated code is running on same server we can use multiple tools and one of them is the docker"
  },
  {
    "file": "premier\\Scalability-files\\Database-Scaling.md",
    "content": "# Database Scaling\n\nwhen our app grows the data we need to store also grows so we need to scale our DB as well. there are a few ways to do it\n\n1. **Vertical scaling**\n   1. It involves adding extra resource (RAM, CPU, storage)\n   2. it also introduces Single point of failure(as all eggs are in same basket)\n   3. It is also expensive and can be scaled up to an extent only.(Hardware and other limitations)\n2. **Indexing**\n   1. Similar to a book index DB index also helps to not scan each row to find the data.\n   2. Indexing is mostly applied to the most queried column. over-indexing can increase the write performance.\n   3. The index holds the columns along with the pointer to the corresponding rows in the table.\n   4. **Working**\n      1. Index creation: DBA creates indexes\n      2. Index building: DBMS created indexes by scanning table\n      3. Query execution: when a query is executed DB engine first sees if an index exist for a queried column\n      4. Index search: if an index is found then it searches data using index pointers\n      5. Data retrieval: retrieved the data found using indexes\n   5. **Types**\n      1. **Based on structure and key Attributes**\n         1. **Primary Index**\n            1. Automatically created when a primary key constraint is defined. ensure uniqueness and fast lookup using primary key.\n         2. **Clustered and non clustered(Secondary) index** [LINK](https://www.geeksforgeeks.org/difference-between-clustered-and-non-clustered-index/)\n            1. Clustered index will change the order of row in the physical table (like primary key will be sorted).\n            2. We can only have one clustered index per table.\n            3. Non Clustered is a normal index\n      2. **Based on Data coverage**\n         1. **Dense index**\n            1. Has entry in table will have an entry in index.\n            2. mostly used in case where we need frequent and fast access.\n            3. primary index also like Dense index (as for each value there is an entry )\n            4. preferred for small tables\n         2. **sparse**\n            1. it contains entry for only some values.\n            2. generally has one entry per page/block.\n            3. used in large tables and clustered tables\n   6. **How to Use effectively**\n      1. index frequently used columns\n      2. index selective columns\n         1. index are most effective in column with good spread of data (like index in gender column will be less beneficial than index on id)\n3. **Sharding**\n   1. a single machine can only store limited data so we need to store data in multiple places.\n   2. It is a type of horizontal scaling used to split large DBs into smaller pieces called shards.\n   3. **Working**\n      1. Sharding Key: it is a unique identifier used to identify which shard data belongs\n      2. Data partitioning: It involves splitting data based on the shard key.\n   4. **Sharding strategies**\n      1. Hash-based\n      2. Range-based: data distributed over a range like dates or numbers.\n      3. Geo-based\n      4. Directory based\n   5. **Benefits**\n      1. Performance\n      2. Scalability\n      3. Geographical distribution\n      4. Cost\n   6. **Complexities**\n      1. Cross shard joins\n      2. Shards have data evenly shared\n   7. **Best Practices**\n      1. Use consistent hashing to avoid data movement during shard addition/deletion\n      2. Choose the right shard key for even distribution\n      3. Handle cross-shard query by data normalization\n4. **Vertical partitioning**\n   1. it is useful when our table contains multiple columns(let’s say A, B, C, D) but column A,B are most freq accessed so we can partition A, B into a separate table so no need to query extra columns.\n5. **Caching**\n   1. Store frequently accessed data in a faster storage layer like in-memory.\n   2. more abt caching will be covered on the caching page\n6. **Replication**\n   1. Maintaining copies of DB across multiple servers.\n   2. Preferred for fault-tolerant, ready heavy, Geographically distributed systems.\n   3. Types\n      1. **Synchronous**\n         1. every write to the primary database is instantly propagated to all replicas. The write operation is only confirmed once all replicas have applied the change.\n         2. slower, guarantees data consistency\n      2. **Asynchronous**\n         1. Changes to the primary database are replicated to replicas with a slight delay. This offers better performance but with the trade-off of potential data inconsistency between the primary and replicas (known as replication lag).\n      3. **Semi-Synchronous Replication:**\n         1. The master waits for at least confirmation from 1 slave\n      4. **Multi-master**\n         1. In multi-master replication, multiple databases act as masters, allowing read and write operations on any node. Each change is propagated to other masters to keep them in sync.\n         2. (e.g., collaborative applications).\n7. **Materialized view**\n   1. Some DB queries are complex and take a long time to execute, so they can slow down performance if called often.\n   2. so we can precompute this. thus Materialized views are pre-computed, disk-stored result sets of complex queries.\n   3. we also need to define a refresh mechanism for these views.\n8. **Data Denormalization**\n   1. Data denormalization is optimizing database performance by intentionally duplicating or aggregating data.\n   2. This is the opposite of data normalization, where data is broken into smaller tables to reduce redundancy and ensure consistency.\n   3. In denormalization, data is consolidated into fewer tables, often sacrificing storage efficiency and some consistency to achieve faster read performance, especially in read-heavy applications.\n   4. **Example**\n      1. suppose we have table customer (c_id,c_name,c_add) and orders (o_id,c_id{foreign key},o_date)\n      2. in the above case get order details with the customer name and address we have to make a join so to avoid this we can add redundant field c_name and c_add to the orders table.\n   5. **Usage**\n      1. Read heavy workload\n      2. Real-time analytics\n"
  },
  {
    "file": "premier\\Scalability-files\\Message-Queue.md",
    "content": "## Message Queues\n\n1. It is a communication mechanism that allows different parts of the system to send and receive messages asynchronously.\n2. It acts as a **intermediary** which holds message sent from producer(or publisher) and delivers them to subscribers(or consumer).\n3. It is a decoupled architecture where publisher and subscribers are not aware of each other.\n\n### **Components**\n\n![Image](../images/message-queue-components.jpg)\n\n1. **Publisher** - entity that sends messages to queue\n2. **Subscriber** - entity that reads message from queue\n3. **Queue** - Data structure that stores message untill they are consumed\n4. **Broker** - It is optional in some. It is a software that manages the message queues, handles that message are routed correctly between consumers and producers.\n5. **Message** - a unit of data sent. generally contains some payload, metadata (headers, timestamps, priority) about the message.\n\n### **How does Message Queues work**\n\n![Image](../images/message-queue-working.jpg)\n\n1. **Sending Message**\n2. **Queueing Message**\n3. Queue stores the message temporarily, making available for one or more consumer\n4. **Consuming Message**\n5. Message consumers retrieve messages from the queue when they are ready to process them. They can do this at their own pace, which enables asynchronous communication.\n6. **Acknowledgement (Optional)**\n7. In some message queue systems, consumers can send acknowledgments back to the queue, indicating that they have successfully processed a message. This is essential for ensuring message delivery and preventing message loss.\n\n### **Types**\n\n1. **Point to Point Queue**\n\n   ![Image](../images/p2p.jpg)\n\n   1. It is simplest type. When a producer sends a message the message is stored in the queue untill the consumer retrieves it.\n   2. Once message is retrieved it is removed from the queue and can not be processed by another consumer.\n   3. **Used in**\n      1. Task processing system\n      2. Log processing system\n      3. Order processing system\n\n2. **Publish/Subscribe (Pub/Sub) Queue**\n\n   ![Image](../images/pub-sub.jpg)\n\n   1. In this model, messages are published to a topic and multiple consumers can subscribe to that topic to receive messages.\n   2. Publishes messages to a topic instead of directly sending them to a queue.\n   3. A Message Broker (Pub/Sub system) is a central system (e.g., Kafka, Google Pub/Sub, Redis) that distributes messages from publishers to all subscribed consumers.\n   4. Subscribers receive messages\n      Any service subscribed to the topic will get the message.\n   5. **Usecase**\n      1. Inventory Service → Updates stock\n      2. Payment Service → Processes the payment\n      3. Shipping Service → Starts delivery\n      4. Notification Service → Sends email/SMS confirmation\n\n3. **Priority Queue**\n\n   1. Messages in the queue are assigned priorities, and higher-priority messages are processed before lower-priority ones.\n   2. **Usecase**\n      1. emergency alerts system\n      2. Healthcare (critical patients alert)\n      3. Customer support system(e.g. premium customer get faster response)\n\n4. **Dead Letter Queue (DLQ)**\n\n   ![Image](../images/dlq.jpg)\n\n   1. Stores messages that could not be processed successfully after multiple retries.\n   2. **Usecase**\n      1. Handeling failed transaction in e-commerce\n      2. Useful for troubleshooting and handeling failed messages\n\n### **Advantages**\n\n1. **Decopling**\n   1. producer and consumer are decoupled\n2. **Ashynchronous Processing**\n   1. producer can send message to queue and move on to other task. similarly consumer can consume message based on availability.\n3. **Fault tolerance**\n   1. Persistent queues ensure that messages not lost even if consumer fails. they also allow reties and error handling.\n4. **Scalability**\n   1. Can scale horizontally adding more consumers and producers\n5. **Throttling**\n   1. This can control rate of message processing, preventing consumer from being overloaded.\n\n### **When to use**\n\n1. **Microservice architecture**\n   1. MS need to communicate to each other but direct calling can lead to tight coupling and cascading failure.\n2. **Task scheduling and Background processing**\n   1. Certain tasks, such as image processing or sending emails, are time-consuming and should not block the main application flow. Offload these tasks to a message queue and have background workers (consumers) process them asynchronously.\n3. **Event driven architecture**\n   1. Event needs to propagated to multiple services but direct communication can lead to tight coupling so we can use a Pub/Sub queue to broadcast event to all interested consumer services.\n4. **Reliable communication**\n   1. Using persistent and retry handeled queue can make reliable communication.\n5. **Load leveling**\n   1. Sudden spikes in requests can overwhelm a system, leading to degraded performance or failures. Queue incoming requests using a message queue and process them at a steady rate, ensuring that the system remains stable under load.\n\n### **Best practices for implementing**\n\n1. **Idempotency**\n   1. Ensure that duplicate messages are handled correctly.\n2. **Message Durability**\n   1. Based on usecase implement persistent or transient messages. as persistent comes with some tradeoff.\n3. **Error Handeling**\n   1. Implement robust error handling by including retirs, DLQ, and alerting mechanism for failed message processing.\n4. **Security**\n   1. Implement security by encryption, authentication.\n5. **Monitoring**\n   1. Setup monitoring to check performance and health of the message queues, including throughput, queue length, and consumer lag.\n6. **Scalability**\n\n### **Popular Message Queue**\n\n1. **Apache Kafka**\n   1. It is distributed streaming platform that excels at handling large volumes of data. Most widely used for high throughput and event driven systems.\n2. **RabbitMQ**\n   1. widely used open source message broker that supports multiple messaging protocols, including AMQP. Supports P2P, Pub/Sub, Priority.\n3. **Google Cloud Pub/Sub**\n   1. A fully managed message queue service offered by Google Cloud, designed for real-time analytics and event-driven applications.\n4. **Amazon SQS**\n   1. A fully managed message queue service provided by AWS. SQS is highly scalable and integrates well with other AWS services.\n"
  },
  {
    "file": "premier\\Scalability-files\\Task-Queue.md",
    "content": "## Task Queues\n\n1. Tasks queues receive tasks and their related data, runs them, then delivers their results. They can support scheduling and can be used to run computationally-intensive jobs in the background.\n\n### **Working**\n\n1. Producer (Main App) submits a task\n2. Task Queue stores the task - The task is temporarily stored in a queue (e.g., Redis, RabbitMQ).\n3. Worker picks up the task - A background worker process listens for tasks and processes them.\n4. Worker executes the task - The task (e.g., sending an email, resizing an image) is executed asynchronously.\n5. Result is stored or sent back - The task result is stored in a database, logged, or sent back to the producer.\n\n### **Types**\n\n1. **Distributed task queues**\n   1. Task distributed across multiple worker nodes for scalability.\n   2. Usecase - High traffic application requiring parallel processing\n2. **Priority task queues**\n   1. Task processed based on priority\n3. **Delayed task queues**\n   1. Task executed after a delay\n   2. usecase - Auto retrying failed mechanisms\n4. **Scheduled task queues**\n   1. Sending daily reports, data backups\n5. **Persistent task queues**\n   1. Financial transactions, Order processing\n\n### **When to use**\n\n1. Background jobs - Email sending, PDF generation, video processing, etc.\n2. Task scheduling - Running periodic jobs\n\n### **Popular**\n\n1. Celery\n2. BullMQ\n"
  },
  {
    "file": "premier\\Scalability.md",
    "content": "# Scalability\n\n[Caches](Scalability-files/Caches.md)\n\n[Clone](Scalability-files/Clone.md)\n\n[Database Scaling](Scalability-files/Database-Scaling.md)\n"
  },
  {
    "file": "premier\\Things-to-Know-when-building-Microservice.md",
    "content": "# Things to Know when building Microservice\n\n1. Micro service architecture (MSA) creates small modular services which belong to single application\n2. One major advantage of MSA is like now independent services can be re-written in different language which is best for service.\n3. we can deploy as well one service without affecting other as long as external interface (API contract) is not modified.\n4. **Issues and resolutions**\n\n   1. **Nano service hell**\n\n      1. it should be like “Billing”, “Payment” service not like “getUserById”, “getUser” service\n      2. point is don’t build too much small services that outweigh the benefits\n\n   2. Use OpenAPI spec in order to make api contract relevant and up to date\n   3. Use Service discovery tools for endpoint finding based on environment\n   4. We can automate deployment jobs using jenkins\n   5. One service fails, everything fails (Ways to mitigate)\n\n      1. Retries - have a retry logic with fixed number of time, use exponential backoff between tries.\n      2. Circuit breaker\n\n         1. if service continuously return error then we need to make a circuit breaker that reduce bombarding request\n            ![https://i1.wp.com/cloudncode.files.wordpress.com/2016/07/circuit-breaker.png](https://i1.wp.com/cloudncode.files.wordpress.com/2016/07/circuit-breaker.png)\n\n   6. **Database per service**\n\n      1. allows flexibility to deploy , elastic search independently per service\n\n   7. **Schema per service**\n\n      1. promote loose coupling\n\n   8. **Data warehousing**\n\n      1. like for each service we have different DB used so for analytical purpose we can choose to give to each service to push data in given format to data house.\n\n   9. **Too many servers, too much cost**\n\n      1. Yes and no, it is true that a monolith service would consume server resources more efficiently and will be easier to write deployment scripts for, however, where MSA really shines is the reduction of everyday costs for development and faster time to market by employing one time extra setup costs which is one of the major reasons to go for MSA to begin with.\n\n   10. **Every service has to support all transport Protocol and authentication**\n\n       1. It is a common requirement for all services to support the common protocol like HTTP and AMQP (Advanced Message Queuing Protocol). and also services which are exposed publicly need to support the Authentication and Authorization.\n       2. This is usually done using some common library which reduce code duplication.\n       3. Solution for this problem is to use a API gateway which is simply a proxy which is aware of all the services registered with it. It take care os supporting all protocols and authentication and communicating with services in a common protocol (mostly HTTP).\n\n          ![Image](https://i2.wp.com/cloudncode.files.wordpress.com/2016/07/api-gateway.png)\n\n       4. Some popular gateways are Apigee, Mashery and Amazon API Gateway\n\n### Service Discovery\n\n1. As in a scalable application there can be dozens of service which scale up and down dynamically thus we need a way to communicate with these.\n2. Service Discovery is a mechanism that allows services to find and communicate with each other in a distributed system\n   e.g. ![Image](./images/service-dis.jpg)\n3. A service registery typically stores\n   1. **Basic:** service name, IP, port, status\n   2. **Metadata:** version, environment, region, tag\n   3. **Health:** status, last health check done on\n   4. **Secure Communication:** certificates, protocola\n4. **Benefits**\n   1. Reduced Manual config\n   2. Risk of failure\n   3. Improved scalability\n5. **Service Registration Options**\n   1. **Manual Registration**\n      1. Done by dev manually, not favoured in scalability point of view.\n   2. **Self Registrations**\n      1. when ever a service starts it register itself to the registery by sending api with all details(name, port, ip,..etc)\n      2. To insure registery upto date, service sends [heartbeat](Uncategorized/Heartbeat.md) signal periodically to let registry know it is active and healthy\n   3. **Third Party Registration**\n      1. Another process handles service registration.\n      2. follows `sidecar` pattern.\n   4. **Automatic Registration by Orchestrators**\n      1. In modern orchestrated environments like Kubernetes, service registration happens automatically. The orchestration platform manages the lifecycle of services and updates the service registry as services start, stop, or scale\n6. **Types of Service Discovery**\n   1. **Client side**\n      1. Here all responsibility is of client (usually a microservice or API gateway) to query registery and route accordingly.\n      2. registery responds with all instances of requested service and then it's on client to route based on usually load balancing algo.\n      3. It is easy to implement and reduces load on central LB\n      4. Netflix's open-source lib **Eureka** is a popular tool for this.\n   2. **Server side**\n      1. Here a central load balancer handles all routing and logic.\n      2. Instead, the client simply sends a request to a central server (load balancer or api gateway), which handles the rest.\n"
  },
  {
    "file": "premier\\Uncategorized\\Bloom-filters.md",
    "content": "# Bloom Filters\n\n1. Suppose we want to query some existance but we are okay with some false positive. Like Netflix don't want to show you already watched movies.\n2. It is kind of a probablistic Data structure which allows us to quickly check whether an element might be in set.\n\n### Components\n\n1. **Bit Array**\n   1. an bit array of size `n` where intially all bits are set to zero.\n2. **Hash functions**\n   1. An hash function takes a input and map it to an index of the bit array.\n   2. In filters we use `k` hash functions, so for each value there will be k index to be mapped in array.\n\n### Working\n\n1. For each value we get k index and mark bits as 1 there.\n2. In lookup we see if all bits for given value's hash are 1 then value is present in set.\n3. If any of the index is off then the value is not present.\n4. There can be false positive as well.\n\n### Things to consider\n\n1. There is one drawback that traditional bloom filters don't support deletion\n2. Size `n` we have to choose correctly, as large can reduce false positives but can increase memory usage.\n3. Higher `k` improves accuracy but slows down insertions and lookups.\n4. For more dynamic datasets we can use Scalable Bloom filters etc.\n\n### Where to Not use\n\n1. Where false positives are not acceptable.\n2. If exact checks are needed\n3. If deleting an element is necessary\n4. Storage is not an issue\n\n### Real world Applications\n\n| **Use Case**         | **Company/Service**               | **Why Bloom Filters?**                |\n| -------------------- | --------------------------------- | ------------------------------------- |\n| Database Indexing    | Google Bigtable, HBase, Cassandra | Avoids unnecessary disk lookups       |\n| Web Caching          | Facebook, Memcached               | Prevents unnecessary database queries |\n| CDN Optimization     | Akamai                            | Reduces backend fetches               |\n| URL Blacklist        | Google Chrome Safe Browsing       | Faster security checks                |\n| Blockchain           | Bitcoin SPV Wallets               | Reduces storage & bandwidth usage     |\n| Password Leak Checks | Have I Been Pwned?                | Checks without exposing passwords     |\n| Spam Filtering       | Gmail, Yahoo Mail                 | Speeds up blocklist lookups           |\n\n### Traditional vs Counting vs Scalable Bloom Filter\n\n| Feature                   | Traditional Bloom Filter (BF) | Counting Bloom Filter (CBF) | Scalable Bloom Filter (SBF)  |\n| ------------------------- | ----------------------------- | --------------------------- | ---------------------------- |\n| **Memory Usage**          | Low                           | Higher (stores counters)    | Dynamic (expands as needed)  |\n| **False Positive Rate**   | Fixed, increases with size    | Fixed, similar to BF        | Controlled, grows with data  |\n| **Supports Deletions?**   | No                            | Yes (uses counters)         | No                           |\n| **Handles Growing Data?** | No (fixed size)               | No (fixed size)             | Yes (expands dynamically)    |\n| **Use Case**              | Fast membership checks        | Cache eviction, DB indexing | Web crawling, large datasets |\n| **Hash Functions Used**   | Multiple                      | Multiple                    | Multiple (per sub-filter)    |\n| **Example Usage**         | Caching, DB queries           | Memcached, DB indexing      | Google Bigtable, log storage |\n\n1. Counting stores data in an array and supports deletion, as in each inserting count is increased and in deletion reduced.\n2. Scalable uses a dynamic array.\n"
  },
  {
    "file": "premier\\Uncategorized\\browser-cache-control.md",
    "content": "# HTTP Cache Control\n\n1. The http `cache-control` header contains some directives in both request and response that control [caching](../Scalability-files/Caches.md) in browser and shared caches(CDN, proxies).\n2. We can only modify `CORS-safelisted response header` with cache-control directives as `Forbidden-request header` can not be updated programatically (user agent update this).\n\n## Cache-Control Directives in Requests vs. Responses\n\n| **Directive**                          | **Request (C → S)** | **Response (S → C)** | **Description & Use Case**                                                                    |\n| -------------------------------------- | ------------------- | -------------------- | --------------------------------------------------------------------------------------------- |\n| **`no-cache`**                         | ✅ Yes              | ✅ Yes               | Forces revalidation before serving cached content. Used when the client wants fresh data.     |\n| **`no-store`**                         | ✅ Yes              | ✅ Yes               | Prevents caching entirely. Used for sensitive data like authentication, banking pages.        |\n| **`max-age=<seconds>`**                | ✅ Yes              | ✅ Yes               | Specifies how long content can be considered fresh. Used to control caching behavior.         |\n| **`s-maxage=<seconds>`**               | ❌ No               | ✅ Yes               | Similar to `max-age`, but applies to shared caches (CDNs, proxies).                           |\n| **`public`**                           | ❌ No               | ✅ Yes               | Allows caching by any cache (browser, proxies, CDNs). Used for static assets.                 |\n| **`private`**                          | ❌ No               | ✅ Yes               | Restricts caching to the end-user’s browser only. Used for user-specific data.                |\n| **`must-revalidate`**                  | ❌ No               | ✅ Yes               | Forces caches to revalidate content before serving stale responses. used along with `max-age` |\n| **`proxy-revalidate`**                 | ❌ No               | ✅ Yes               | Forces shared caches (CDNs, proxies) to revalidate stale content.                             |\n| **`no-transform`**                     | ❌ No               | ✅ Yes               | Prevents caches from modifying content (e.g., image compression by proxies).                  |\n| **`stale-while-revalidate=<seconds>`** | ❌ No               | ✅ Yes               | Allows serving stale content while revalidating in the background.                            |\n| **`stale-if-error=<seconds>`**         | ❌ No               | ✅ Yes               | Serves stale content when the origin server is down.                                          |\n| **`immutable`**                        | ❌ No               | ✅ Yes               | Indicates that a resource will not change while cached. Used for versioned assets.            |\n\n### Note\n\n`immutable` and `stale-while-revalidate` are not have all browser support.\n\n## Common used patterns\n\n### Cache Static assests\n\n```sh\nCache-Control: public, immutable, max-age=432432434\n```\n\n1. used for versioned assets.(e.g. logo-v1.png,app-v2.js)\n2. public: Allows caching for all (CDN, browser, proxies)\n3. immutable: Did this so no need to revalidate by browser\n\n### Prevent Caching entirely\n\n```sh\nCache-Control: no-store\n```\n\n**Usecase:** Login pages, Transaction pages\n\n### Ensure fresh data always\n\n```sh\nCache-Control: no-cache, must-revalidate\n```\n\n**Usecase:** Dashboard analytics\n\n### Serve cached content if server failure\n\n```sh\nCache-Control: public, max-age=600, stale-if-error=3600\n```\n\n**Usecase:** For good user experience\n\n### Optimise content delivery on cdn\n\n```sh\nCache-Control: public, max-age=600, s-maxage=86400\n```\n\nCDN caches content for 1 day and browser for 10 mins.\n"
  },
  {
    "file": "premier\\Uncategorized\\client-server-architecture.md",
    "content": "# Client Server Architecture\n\n1. In this multiple clients (users or devices) interact with a centralized server to access data, resource, or services.\n\n## Types\n\n### Two-Tier\n\n1. Here client directly interact with server.\n2. The server typically handles both application logic and data management.\n3. Simple but ineffecient as number of client increases\n4. e.g. a desktop app that directly connects to DB server to retrieve and show data.(MongoDB compass)\n\n### Three-Tier\n\n1. Here Database are handleded on differe server\n2. client -> App Logic server -> DB server\n3. e.g. a web app which follow it.\n\n### N-Tier\n\n1. extension of 3 tier model, where additional layer can be caching, load balancing, security.\n\n## Advantages\n\n1. Centralized Management\n2. Scalability\n3. Resource sharing\n4. Security\n\n## Challenges and Considerations\n\n1. SPOF - if server down all client will be affected\n2. Complex to manage\n"
  },
  {
    "file": "premier\\Uncategorized\\consensus-in-ds.md",
    "content": "# Consensus in Distributed Syetem\n\n1. In a DS multiple nodes are mutually connected and collaborate with each other through message passing. Now during some computation they need to agree upon a common value to co-ordinate among multiple process. This phenomenon is called **Distributed Consensus**.\n2. In a DS it may happen multiple nodes are processing a large computation where they need to know the result of each other to keep them updated about the whole system.\n\n## Why needed with example\n\n1. Suppose we have a banking system and we processed like transfer amount `x` from accountA to accountB. The system must ensure all servers agree on value `x`.\n2. **Problems we can have without consensus**\n   1. server1 updated accountA to `-x` but crashed before updating accountB to `+x`\n   2. server2 sees an inconsistent state.\n   3. if nodes not agree on a single correct state, incorrect baance might occur.\n3. **How Consensu solve this**\n   1. All nodes first agree on a single order of transaction\n   2. Even if some node fail the majorty can still ensure correctness\n   3. No transaction lost and all nodes eventually reach same state\n4. It is also used for Distributed DBs to ensure ACID properties\n\n## How to achieve Consensus in DS\n\n```\n* Nonfaulty node means, nodes which is not crashed or attacked or malfunctioning.\n```\n\n1. All nonfaulty nodes should agree on a same value `v` if one of them not agree then consensus can not be achieved\n2. The value `v` should be proposed by a non-faulty node\n\n## Challenges\n\nMost common\n\n1. **Crash**\n   1. It occurs when a node is not responding to other nodes of the system due to some Hardware or Software ot Network fault.\n   2. It can be handles easily by ignoring node's existance.\n2. **Byzantine failure**\n   1. A situation where one or more node is not crashed but behaves abnormally and forward a different message to different peers, due to an internal or external attack on that node. Handling this kind of situation is complicated in the distributed system.\n   2. A node may act maliciously, sending false information to other nodes (e.g., hacking attempts in blockchain).\n   3. A consensus algorithm, if it can handle Byzantine failure can handle any type of consensus problem in a distributed system.\n\n## Consensus Algorithms\n\n### Voting based\n\n1. **Practical Byzantine Fault Tolerance**\n   1. It handles Byzantine failure\n   2. It works on principle **If more than two-thirds of all nodes in a system are honest then consensus can be reached.**\n   3. **Working**\n      1. The client sends a request to the primary node.\n      2. The primary nodes broadcast the request to all secondary nodes.\n      3. All the nodes perform the service that is requested and send it to the client as a reply.\n      4. The request is served successfully when the client received a similar message from at least two-thirds of the total nodes.\n   4. **Pros**\n      1. Can tolerate up to 1/3 of nodes being faulty.\n   5. **Cons**\n      1. Communication overhead\n2. **Paxos**\n   1. Paxos is one of the most widely used consensus algorithms. It is a fault-tolerant, majority-based consensus protocol.\n   2. Used in Google Spanner\n   3. **Working**\n      1. Proposer proposes a value.\n      2. Acceptors (Majority of nodes) vote on the proposal.\n      3. If a majority of acceptors agree, the value is committed and learned by all nodes.\n   4. **Pros**\n      1. Fault tolerant\n   5. **Cons**\n      1. Involves multiple message exchanges\n3. **Raft**\n   1. It uses leader election\n   2. Used in Cockrock DB\n   3. **Working**\n      1. A leader is elected among nodes.\n      2. The leader receives updates and replicates them to followers.\n      3. Once a majority of nodes confirm, the update is applied.\n   4. **Pros**\n      1. Simpler than Paxos\n   5. **Cons**\n      1. If leader fails a new election must happen\n\n### Proof based\n\n1. **Proof of Work**\n   1. Used in bitcoin and blockchain.\n   2. **Working**\n      1. A node (miner) solves a complex mathematical puzzle\n      2. The solution (proof) is verified by other nodes.\n      3. If valid, the block is added to the blockchain.\n   3. **Pros**\n      1. Highly secure\n   4. **Cons**\n      1. Slow\n      2. Consume huge energy\n2. **Proof of Stake**\n   1. Used in Etherium 2.0\n   2. PoS is an alternative to PoW where validators stake coins instead of solving puzzles.\n"
  },
  {
    "file": "premier\\Uncategorized\\distributed-locks.md",
    "content": "# Locking (Normal and Distributed)\n\n## Normal Locking (Single Node Locking)\n\nIn a single node system locking is used when multiple thread tried to access the critical system at a same time.\n\n### Types\n\n1. **Optimistic**\n   1. Assumption is made conflicts are rare and proceeds without locking.\n   2. Before commiting changes, it checks whether another process has modified the resouce.\n   3. It is a strategy where you read a record, take note of a version number and check that the version hasn’t changed before you write the record back.\n   4. **Implementation:** Uses versioning to validate\n   5. **Usecase**\n      1. PostgreSQL uses Multi version concureency control (MVCC)\n2. **Pessimistic**\n   1. Assumes that if something can go wrong means it will go wrong so lock the resource before performing any operation.\n   2. **Implementation:** use DB row level locks or thread sync primitives(`mutex`,`semaphore`).\n   3. **Usecase**\n      1. Banking system\n\n## Distributed Locking (Multi Node Locking)\n\nDistributed locking is a mechanism used to coordinate access to shared resources in a distributed system where multiple nodes (services, processes, or servers) might try to modify the same data concurrently.\n\n### Types\n\n1. **Optimistic**\n\n   1. Assumption is made conflicts are rare and allows multiple nodes to proceed with an operation without explicit locking.\n   2. **Implementation:**\n\n      ```\n          -- Step 1: Read the record\n          SELECT id, value, version FROM items WHERE id = 1;\n\n          -- Step 2: Try updating the record with version check\n          UPDATE items\n          SET value = 'newValue', version = version + 1\n          WHERE id = 1 AND version = 3; -- Only updates if version is still 3\n      ```\n\n   3. **Usecase**\n\n      1. NoSQL databases (Cassandra, DynamoDB, etc.) use versioning for concurrent updates.\n      2. E-commerce applications where multiple users try to purchase limited stock.\n      3. API rate limiting (checking counters with atomic operations).\n\n   4. **Advantages**\n\n      1. No need for explicit locks\n      2. Scales well in DS\n\n   5. **Disadvantages**\n\n      1. Not suitable for frequent locks\n      2. High contention can lead to many retries\n\n2. **Pessimistic**\n\n   1. Assumes conflicts are frequent and prevents other processes from modifying a resource while one process is working on it.\n   2. Uses explicit locking to prevent concurrent modifications.\n   3. Implemented using external distributed coordination systems like **Redis**, **ZooKeeper**, or **database locks**.\n\n### Redis Based Distributed Locking\n\n1. **Working**\n   1. A process requests a lock from Redis using the `SET NX PX` command (Set if Not Exists + Expiry).\n   2. If the lock exists, other processes must **wait or retry**.\n   3. The process releases the lock after completing the operation.\n2. **Issues**\n   1. If a system crashes before releasing the lock, the resource remains locked.\n   2. Solution to above is using `Redlock Algo` whic used multiple nodes and work as a dirtributed locking mechanism.\n3. **Redlock Algorithm**\n   1. The process acquires a lock from at least `N/2 + 1` Redis nodes to confirm exclusivity.\n   2. If a node fails, the system still functions.\n   3. Ensures fault tolerance.\n\n### Database Based Distributed Locking\n\n1. **Working**\n\n   1. Use a database table to store lock information.\n   2. A process inserts a row representing the lock.\n   3. Other processes wait until the row is released.\n\n2. **Implementation**\n\n   ```\n        -- Acquire lock\n        INSERT INTO distributed_locks (lock_name, acquired_at)\n        VALUES ('order_processing', NOW())\n        ON CONFLICT DO NOTHING;\n\n        -- Release lock\n        DELETE FROM distributed_locks WHERE lock_name = 'order_processing';\n\n   ```\n\n3. **Issues**\n\n   1. Deadlocks if locks are not released.\n   2. Performance bottleneck in high-concurrency scenarios.\n   3. Single point of failure if using a single database instance.\n\n### ZooKeeper-Based Distributed Locking\n\nApache **ZooKeeper** is a **highly available distributed coordination system** used for leader election, service discovery, and distributed locking. It provides **strong consistency** and is widely used in large-scale systems.\n\n1. **Working**\n\n   1. ZooKeeper uses a concept called **znodes** (ZooKeeper nodes) for storing data in a hierarchical structure, similar to a filesystem.\n   2. **Step 1: Clients Create Ephemeral Sequential znodes**\n\n      1. Each process creates a **znode** under the `/locks` directory.\n      2. | Process | Znode Created  | Znode Path            |\n         | ------- | -------------- | --------------------- |\n         | **P1**  | `lock-0000001` | `/locks/lock-0000001` |\n         | **P2**  | `lock-0000002` | `/locks/lock-0000002` |\n         | **P3**  | `lock-0000003` | `/locks/lock-0000003` |\n      3. Sequential znodes ensure an ordering mechanism.\n      4. Ephemeral znodes disappear if the client disconnects (prevents deadlocks).\n\n   3. **Step 2: Finding the Process with the Smallest znode**\n\n      1. Each process lists all znodes under `/locks`.\n      2. The process with the smallest number (`lock-0000001`) gets the lock.\n      3. **P1 acquires the lock** and starts writing to the file.\n\n   4. **Step 3: Other Processes Watch the Next Smallest Znode**\n\n      1. P2 watches **`lock-0000001`**.\n      2. P3 watches **`lock-0000002`**.\n\n   5. **Step 4: Lock Release and Next Process Acquires It**\n\n      1. Once P1 finishes, it deletes `/locks/lock-0000001`.\n      2. P2 now acquires the lock and starts writing.\n\n2. **Advantages**\n\n   1. **Strong Consistency:** Only one process gets the lock at a time.\n   2. **Automatic Deadlock Prevention:** Ephemeral znodes **auto-delete** if a process crashes.\n   3. **Scalability:** Works in **distributed systems** with multiple nodes.\n   4. **Sequential Ordering:** Ensures **fair queuing** of lock requests.\n\n3. **Challenges:**\n\n   1. Requires a **ZooKeeper quorum** (at least **3 nodes**) for high availability.\n   2. **More overhead** than Redis locks due to disk persistence.\n\n4. **Usecase**\n\n   1. **Leader election** in distributed systems.\n   2. **Ensuring exclusive access** to shared resources.\n   3. **Ensuring correct order of execution** in queue-based systems (Kafka, Hadoop, etc.).\n   4. **Critical operations where consistency is a must** (e.g., distributed databases).\n"
  },
  {
    "file": "premier\\Uncategorized\\gossip-protocol.md",
    "content": "# Gossip protocol\n\n1. Some problem that we face in a DS are\n   1. maintaining the system state(liveness of nodes)\n   2. Communication between nodes\n2. Potential solution to these problems\n   1. Centralized state management service\n   2. p2p state management service\n\n## Centralized State management service\n\n1. We can use something like Apache Zookeeper as a [service discovery](../Things-to-Know-when-building-Microservice.md#service-discovery) to keep track of state of every node in the system.\n2. It is good for CP in CAP thorem but it introduces SPOF and had scalability issue.\n\n## Peer to Peer State management service\n\n1. It is inclined toward AP in CAP thorem and gives eventual consistency.\n2. The gossip protocol algos can be used to implement p2p state management service with high scalability and improved resilience.\n3. The **gossip protocol** is also known as the **epidemic protocol** because the tranmission of message is similar to the way how epidemics spread.\n\n## Broadcast Protocols\n\n### Point-to-Point broadcast\n\nProducer consumer model\n\n### Eager reliable broadcast\n\nEvery node re-broadcasts the messages to every other node via reliable network links. This approach provides improved fault tolerance because messages are not lost when both the producer and the consumer fail simultaneously. The message will be re-broadcast by the remaining nodes. The caveats of eager reliable broadcast are the following:\n\n1. significant network bandwidth usage due to O(n²) messages being broadcast for n number of nodes\n2. sending node can become a bottleneck due to O(n) linear broadcast\n3. every node stores the list of all the nodes in the system causing increased storage costs\n\n### Gossip protocol\n\n<p align=\"center\">\n    <img src=\"../images/gossip.gif\"/>\n</p>\n\n1. The gossip protocol is a decentralized peer-to-peer communication technique to transmit messages in an enormous distributed system.\n2. The key concept of gossip protocol is that every node periodically sends out a message to a subset of other random nodes. The entire system will receive the particular message eventually with a high probability.\n3. The gossip protocol is a technique for nodes to build a global map through limited local interactions.\n4. The gossip protocol built on a robust, scalable and eventual consistent algorithm. It is reliable as if one node fail the message will be delivered by another node.\n5. Why gossip is a optimal choice in large distributed system\n   1. limits the number of messages tranmitted by each node\n   2. tolerate network and node failures\n\n## Working of Gossip Protocols\n\n1. **Node Selection**: Each node periodically selects a few random nodes from the network.\n2. **Information Exchange:** The selected nodes share updates or data (e.g., membership lists, system status, or events).\n3. **Propagation:** The receiving nodes then relay the received information to another set of random nodes in subsequent rounds.\n4. **Convergence:** After several rounds, all nodes in the network will have received the update, ensuring eventual consistency.\n\n## Types of Gossip Protocols\n\n### Anti entropy (full sync)\n\nNodes randomly select peers to exchange thie entire datasets, correcting any discrepancies. It takes extra bandwidth as whole dataset is sent.\n\n### Rumor Mongering (probablistic propagation)\n\nNodes spread new information to random peers, who then decide probabilistically whether to continue spreading it.\n\n### Epidemic-style membership\n\nNodes share membership information to maintain an updated list of active nodes, facilitating dynamic network topology.\n\n### Agreegation based\n\nNodes exchange partial data to compute global aggregates like averages or sums.\n\n## Strategies to Spread a Message through Gossip Protocol\n\n### Push based\n\nThe sender selects random peers and sends the message to them. Those peers then continue spreading the message to other random peers.\n\n1. **Pros**\n   1. Simple to implement\n   2. works well for rapidly spreading new updates\n2. **Cons**\n   1. can lead to redundant message sent\n   2. might take longer to reach all nodes in large networks\n\n### Pull based\n\nNodes periodically contact random peers and ask if they have any new messages. If a peer has a new message, it shares it.\n\n1. **Pros**\n   1. More efficient for rare updates (less unnecessary message flooding).\n   2. Reduces redundant messages.\n2. **Cons**\n   1. Updates may take longer to propagate compared to push gossip.\n\n### Push-Pull based (Hybrid)\n\nNodes both push updates to random peers and pull updates from them.\n\n1. **Pros**\n   1. Faster convergence since updates spread in both directions.\n   2. Efficient in detecting missing information.\n2. **Cons**\n   1. Can generate slightly more network traffic than pure pull gossip.\n\n## Applications\n\n1. **Distriuted Databases** - ensure eventual consistency and data replication\n2. **Blockchain and Cryptocurrencies** - Propagates transactions and blocks efficiently across decentralized networks.\n3. leader election\n\n## Disadvantages\n\n1. Eventual consistency\n2. Bandwith consumption\n3. Unawareness of network patition\n4. difficulty in debugging and testing\n\n## Mathematical Assurance of Eventual Delivery\n\n1. In a network of `N` nodes, if each node gossips to `k` (called **fanout**)random peers per round, the number of informed nodes grows exponentially.\n2. After `log(N) base k` rounds, nearly all nodes will have received the message.\n3. Example: In a network of 1,000 nodes, even if each node gossips to only 3 nodes per round, most nodes will be informed in ~10 rounds.\n"
  },
  {
    "file": "premier\\Uncategorized\\Heartbeat.md",
    "content": "# Heartbeat\n\n1. In distributed system, it is a periodic message sent from one component to monitor each other health status\n\n### Why Needed\n\n1. Monitoring\n2. Detecting failures\n3. Load balancing\n4. Triggering recovery actions\n\n### Working\n\n1. The sender(node) sends message to receiver(monitor) at a regural interval.\n2. if receiver don't receive message after Timeout specifiedś it mark node as failed/unavailable.\n3. Then system can take appropriate actions like, re-route traffic, alert...\n4. **There are some nuances**\n   1. **Frequency**\n      1. should be optimal, not too less not much\n   2. **Timeout**\n      1. it depends on app need\n   3. **Payload**\n      1. generally it contain small info, but it can also contain current load, health metrics, version etc.\n\n### Types\n\n1. Push\n2. Pull\n\n### Challenges\n\n1. Network congestion\n2. Resource usage\n3. False positive - Poorly configured heartbeat intervals might lead to false positives in failure detection, where a slow but functioning component is incorrectly identified as a failed one.\n\n### Real world examples\n\n1. **Database Replication:** Primary and replica databases often exchange heartbeats to ensure data is synchronized and to trigger failover if the primary becomes unresponsive.\n\n2. **Kubernetes:** In the Kubernetes container orchestration platform, each node sends regular heartbeats to the control plane to indicate its availability. The control plane uses these heartbeats to track the health of nodes and make scheduling decisions accordingly.\n\n3. **Elasticsearch:** In an Elasticsearch cluster, nodes exchange heartbeats to form a gossip network. This network enables nodes to discover each other, share cluster state information, and detect node failures.\n"
  },
  {
    "file": "premier\\Uncategorized\\Idempotency.md",
    "content": "# Idempotency in Distributed System\n\n<p>\n   <img src=\"../images/idempotency.webp\" />\n</p>\n\n1. In certain scenarios we need a operation to produce same result even if it is called multiple times. this property is called Idempotency.\n2. It is used as suppose for a payment processing system the call fails for client but it is submitted successfully on Backend, then when user will retry the request will be submitted again and user will be charged twice.\n\n## Ways to Implement\n\n### Unique Request Identifier with Idempotency Key\n\n1. **Working**\n   1. The client generates a unique key (may be UUID) for each operation and send it in the request header.\n   2. Server stores the key and response in a central DB like redis (for DS)\n   3. if a request with same key found again server return response that is cached instead of procesing again.\n2. **Usecase**\n   1. API based transactions\n   2. retry handeling\n3. **Disadv**\n   1. Require Persistent storage\n   2. Needs key expiry handeling\n\n### In messaging system\n\n1. In a messaging system, we can enforce idempotency by storing a log of processed message IDs and checking against it for every incoming message.\n2. Each message has a unique messageId. Before processing, we check if the messageId is already in processedMessages. If it is, the message is ignored; otherwise, it’s processed and added to the set to avoid duplicates.\n\n### In HTTP Methods\n\n1. GET, PUT, DELETE are idempotent\n2. POST is not idempotent\n\n### Some other ways\n\n1. Enforce unique constraints on database fields (e.g., unique transaction_id). If a duplicate request tries to insert the same record, the DB rejects it.\n2. Instead of a client-generated key, the server generates a hash of the request payload. If the same hash is detected again, the request is considered a duplicate.\n3. Before processing a request, a lock is acquired. If the same request is received while the lock is active, it is rejected.\n\n## Challenges and Consideration\n\n1. **Distributed Systems:** Ensuring idempotency across distributed systems can be challenging and may require distributed locking or consensus algorithms.\n2. **Time Window:** How long should idempotency guarantees be maintained? Forever, or for a limited time?\n3. **Database Constraints:** Not all operations are idempotent by default; unique constraints or upsert logic may be necessary to avoid duplication.\n4. **Performance Overhead:** Storing idempotency keys or checking for duplicate operations can add overhead and increase the overall latency.\n\n## Best Practices\n\n1. Use Unique Identifier (UUID or user-based)\n2. Design for idempotency from start\n3. Implement retry with backoff\n"
  },
  {
    "file": "premier\\Uncategorized\\serverless-arch.md",
    "content": "# Serverless Architecture\n\n1. It is a approach that allows developers to build and run services without having to manage the underlying infra.\n2. Developers just write thier code and deploy, while a cloud provider provisions server to run their applications, databases and storage systems at any scale.\n\n## Working\n\n1. One of the most popular serverless architectures is Function as a Service (FaaS), where developers write their application code as a set of discrete functions. Each function will perform a specific task when triggered by an event, such as an incoming email or an HTTP request. After the customary stages of testing, developers then deploy their functions, along with their triggers, to a cloud provider account. When a function is invoked, the cloud provider either executes the function on a running server, or, if there is no server currently running, it spins up a new server to execute the function. This execution process is abstracted away from the view of developers, who focus on writing and deploying the application code.\n2. There are some other popular are serverless containers.\n\n<p>\n<img src='../images/faas.jpg'/>\n</p>\n\n## Benefits\n\n1. Cost efficiency (Pay per use)\n2. Scalability (Auto scaling)\n3. Developer Productivity (Focus on code)\n4. Resilience and Availability\n   1. each function/container run in isolation\n   2. built-in fault tolerance\n\n## Challenges and Considerations\n\n1. **Cold start latency**\n   1. First invocation of a function after a long period of inactivity can experience a higher latency.\n   2. Can be mitigated by using techinques like function warmer, Provisioned concurrency.\n2. **Complexity in State Management**\n   1. Since serverless functions are stateless, managing state across multiple sessions can be challenging.\n3. **Vendor Lock-In**\n   1. Heavily rely on services provided by a vendor.\n   2. can mitigate using a open source serverless framework like Knative.\n4. **Debugging and Monitoring**\n5. **Resource Limits**\n   1. Serverless functions generally have execution time limit(e.g. 15 minutes for AWS lambda).\n\n## Common Usecase\n\n1. Web application\n2. Data processing\n3. Event-Driven Automation: Serverless functions can automate routine tasks such as backups, monitoring, and notifications based on predefined events.\n4. Devops and CI/CD\n\n## Examples\n\n1. Google Cloud Functions\n2. AWS Lambda\n3. Micrisoft Azure functions\n"
  },
  {
    "file": "premier\\what-happens-when-hit-url.md",
    "content": "# What Happens when we hit URL in browser\n\n<p align=\"center\">\n   <img src=\"./images/HTTP-high-level-workflow.webp\"/>\n</p>\n\n## Steps\n\n1. [DNS Resolution](./DNS-CDN-Load_balancer-Proxies.md#domain-name-system)\n2. **TCP three-way Handshake**\n\n   1. The client should create a connection to the server to transfer and receive data. TCP is one of the underlying protocols in Hypertext Transfer Protocol (HTTP).\n   2. The client performs a three-way handshake with the server to establish a TCP connection. TCP requires a three-way handshake because of the bi-directional communication channel. If you make a two-way handshake, you can only start a single-directional communication channel.\n   <p align=\"center\">\n      <img src=\"./images/TCP-three-way-handshake.webp\"/>\n   </p>\n\n   3. The following synchronize (SYN) and acknowledge (ACK) messages are sent between the client and the server to open a TCP connection:\n      1. The client sends an SYN request with a random sequence number (x).\n      2. The server responds with SYN-ACK. The acknowledgment number is set to one more than the received sequence number (x+1). The server sends another random sequence number (y).\n      3. The client sends ACK. The client sends an acknowledgment number that is one more than the received sequence number (y+1).\n"
  },
  {
    "file": "Questions\\cdn.md",
    "content": "# Content Delivery Network\r\n\r\n## Requirements\r\n\r\n### Functional\r\n\r\n1. Serve static content(JS, CSS, images, videos) efficiently\r\n2. Reduce latency for users across diff geo locations\r\n3. provide caching, LB, and failover mechanism\r\n4. Support realtime content invalidation and failover mechanism\r\n5. Write operation for dynamic content\r\n6. Efficient propagation of new Content, with cache invalidation(if file is updated then new cache should update immediately)\r\n\r\n### Non-Functional\r\n\r\n1. Scalable\r\n2. Low latency\r\n3. Security\r\n\r\n## Capacity Estimation\r\n\r\n1. Total users - 100M\r\n2. Average content size is 500KB\r\n3. read/write ratio : 100 : 1\r\n\r\n### Traffic\r\n\r\n| Desc       | value                          |\r\n| ---------- | ------------------------------ |\r\n| QPS(read)  | 100M\\*(24*60*60) = 1200 approx |\r\n| QPS(write) | 12                             |\r\n\r\n### Storage\r\n\r\n| Desc               | value                                     |\r\n| ------------------ | ----------------------------------------- |\r\n| storage per day    | 500KBx1M = 500 GB (excluding replication) |\r\n| storage for 1 year | 181 TB                                    |\r\n\r\n### Memory(Cache)\r\n\r\nIf we follow 80:20 rule for caching\r\n\r\n| Desc            | value                           |\r\n| --------------- | ------------------------------- |\r\n| storage per day | 500KBx100M = 50 TB\\*0.8 = 40 TB |\r\n\r\n## High Level Design\r\n\r\n<p align=\"center\">\r\n   <img src=\"./images/cdn/hld.svg\" />\r\n</p>\r\n\r\n### Edge Servers (Point of Presence)\r\n\r\n1. It is a server which a availabe near to the user\r\n2. It caches the data and reduce load on origin server by fullfilling request at own\r\n3. If content not found then request Origin server\r\n\r\n### Origin Servers\r\n\r\n1. It is the main server, Original storage for the content\r\n2. If cache miss happens at the Edge server then content served from here\r\n3. Handles content upload\r\n\r\n### Global Traffic Manager (Geo-DNS Load balancer)\r\n\r\n1. It routes the user request nearest and least loaded edge server\r\n2. Handles the failure by if one edge server down the redirect to other edge server\r\n3. Balance load across all location\r\n4. Based on geographic proximity routes the user to nearest edge server, few approah can be used\r\n   1. GeoDNS - based on IP\r\n   2. Anycast routing - network itself handles nearest node based on BGP(Border gateway protocol)\r\n\r\n### Multi-tier Caching\r\n\r\n1. Reduce the latency by stroring content at diff cahcing layers like\r\n   1. Client side browser cache\r\n   2. Edge caches(POPs)\r\n   3. Regional Cache - used when pop cache miss but before reaching origin\r\n   4. Origin Cache\r\n2. **Cache Invalidation**\r\n   1. TTL\r\n   2. Event based (on pull remove old)\r\n   3. Versioning - use diff urls for updated content\r\n\r\n### Content Replication\r\n\r\n1. Ensure every update to origin is propogated across all cdn\r\n2. **Mehods**\r\n   1. Push\r\n   2. Pull\r\n   3. Hybrid - push popular content and pull less popular\r\n\r\n### Security\r\n\r\n1. Avoid DDos by rate-limiting\r\n2. Token based auth for provate content\r\n3. SSL/TLS encryption for secure delivery\r\n\r\n### Read flow\r\n\r\n<p align=\"center\">\r\n    <img src=\"./images/cdn/mermaid-diagram-read-flow.svg\"/>\r\n</p>\r\n\r\n### Write flow\r\n\r\n<p align=\"center\">\r\n    <img src=\"./images/cdn/mermaid-diagram-write-flow.svg\"/>\r\n</p>\r\n\r\n## DB Design\r\n\r\n### Content metadata (Not clear still)\r\n\r\nStores metadata about CDN-stored content.\r\n\r\n```\r\nCREATE TABLE content_metadata (\r\n    id SERIAL PRIMARY KEY,\r\n    url TEXT UNIQUE NOT NULL,  -- CDN URL for the content\r\n    origin_url TEXT NOT NULL,  -- Original source URL\r\n    ttl INT DEFAULT 3600,  -- Time-to-live in seconds\r\n    size BIGINT,  -- Size of content in bytes\r\n    version INT DEFAULT 1,  -- Versioning for updates\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\r\n);\r\n```\r\n\r\n## API Design\r\n\r\n### Get data from cdn\r\n\r\n```sh\r\nGET /cdn/content/{content_id}\r\n\r\nResponse: {\"cdn_url\": \"<cdn_url>,\"cache_hit\": true}\r\n\r\n```\r\n\r\n### Upload to cdn\r\n\r\n```sh\r\nPOST /cdn/upload\r\nbody : {\"file\":binary}\r\nauth token\r\n\r\nResponse: {\"status\":OK, \"cdn_url\": \"<cdn_url>,\"content_id\": string}\r\n\r\n```\r\n\r\n### Update content (Invalidation)\r\n\r\n```sh\r\nPUT /cdn/content/{content_id}\r\nbody : {\"file\":binary,\"version\":v2}\r\nauth token\r\n\r\nResponse: {\"status\":OK, \"version\": \"v2\",\"content_id\": string}\r\n\r\n```\r\n\r\nPENDING\r\n\r\n1. DB design\r\n"
  },
  {
    "file": "Questions\\leader-board.md",
    "content": "# Leaderboard\n\n## Requirements\n\n### Functional\n\n1. User should be able to see top 10 players based on score\n2. Can see specific player's rank as well\n3. Can see surrounding players rank if search a specific player\n4. leaderboard can rank player's based on daily, monthly (time filter)\n5. Can see historic match leaderboard as well\n6. Leaderboard can be updated in full distributed manner\n7. Should support thousand of players\n8. Need realtime\n\n### Non functional\n\n1. High availability\n2. Low latency\n3. Scalability\n4. Reliability\n\n## Estimation\n\n1. Users are globally distributed\n2. read/write ratio - 5:1\n3. Daily active users for writes - 50 Million\n4. Player id can be 30 bytes string and score can be 16 bit int consuming 2 bytes.\n\n### Traffic\n\n| Desc           | value                                     |\n| -------------- | ----------------------------------------- |\n| DAU (write)    | 50 M                                      |\n| QPS(write)     | (50M)/(sec in one day) = 587 (600 approx) |\n| QPS(read)      | 600\\*5 = 3000                             |\n| Peak QPS(read) | 4000 (assumption)                         |\n\n### Memory (for fater use)\n\n| Desc                         | value             |\n| ---------------------------- | ----------------- |\n| total player                 | 50 M              |\n| Single player record         | (30+2) = 32 bytes |\n| total storage for all player | 50M\\*32 = 1.6 GB  |\n\n### Storage\n\n| Desc                                     | value                     |\n| ---------------------------------------- | ------------------------- |\n| storage for a day all player             | 1.6 GB                    |\n| total storage for all player for 5 years | 1.6 GB * 5 *365 = 2.92 TB |\n\n### Bandwidth\n\n**Ingress** is the network traffic that enters the server (client requests) and **Egress** is the network traffic that exits the server(server resp)\n\n| Desc    | value                                                                    |\n| ------- | ------------------------------------------------------------------------ |\n| Ingress | 32 bytes/player _ 50M players/day _ 10^-5 day/sec = 16 KB/sec            |\n| Egress  | 64 bytes/player _ 250 million players/day _ 10^(-5) day/sec = 160 KB/sec |\n\n### API design\n\n1. for each of scalability we will be using REST. as it allows loose coupling as than RPC.\n\n#### Update player score\n\n1. Request\n   ```\n   /players/:player-id/scores\n   method: POST\n   authrorization: Bearer <token>\n   content-length: 100  (size of the request body in bytes)\n   content-type: application/json\n   content-encoding: gzip (lossless encoding for reduced data size)\n   {\n       player_id:<int>,\n       score:<int>,\n       location:geohash\n   }\n   ```\n2. Response\n   ```\n   200 - success\n   202 - accepted for async processing\n   403 - Unauth\n   400 - invalid payload\n   ```\n\n#### Get a single player score\n\n1. Request\n\n   ```\n   /players/:player-id\n   method: GET\n   authrorization: Bearer <token>\n   accept: application/json, text/html\n   user-agent: chrome\n   ```\n\n2. Response\n\n   ```\n   status code: 200 OK\n   cache-control: private, no-cache, must-revalidate, max-age=5\n   content-encoding: gzip\n   content-type: application/json\n\n   {\n       player_id: <string>,\n       player_name: <string>,\n       score: <int>,\n       rank: <int>,\n       updated_at: <date_timestamp>\n   }\n   ```\n\n   ```\n   403 - Unauth\n   404 - not found\n   ```\n\n#### Get top N player\n\n1. Request\n\n   ```\n   /players/top/:count\n   method: GET\n   authrorization: Bearer <token>\n   accept: application/json, text/html\n   user-agent: chrome\n   ```\n\n2. Response\n\n   ```\n   status code: 200 OK\n   cache-control: public, no-cache, must-revalidate, max-age=5\n   content-encoding: gzip\n   content-type: application/json\n\n   {\n       count: 10(count),\n       updated_at:<timestamp>,\n       data: [\n           {\n               player_id: <string>,\n               player_name: <string>,\n               score: <int>,\n               rank: <int>\n           }\n       ]\n   }\n   ```\n\n   ```\n   403 - Unauth\n   ```\n\n#### Get surrounding players score\n\n1. Request\n\n   ```\n   /players/:player-id/:count\n   method: GET\n   authrorization: Bearer <token>\n   accept: application/json, text/html\n   user-agent: chrome\n   ```\n\n2. Response\n\n   ```\n   status code: 200 OK\n   cache-control: public, no-cache, must-revalidate, max-age=5\n   content-encoding: gzip\n   content-type: application/json\n\n   {\n       count: 10(count),\n       updated_at:<timestamp>,\n       data: [\n           {\n               player_id: <string>,\n               player_name: <string>,\n               score: <int>,\n               rank: <int>\n           }\n       ]\n   }\n   ```\n\n   ```\n   403 - Unauth\n   ```\n\n#### Service health\n\n```\n/:service/health\nmethod: HEAD\n```\n\nResponse\n\n```\n200 - OK\n500 - error\n```\n\n### DB\n\n### SQL schema\n\n<p align=\"center\">\n    <img src=\"./images/leaderboard/leaderboard-schema-sql.svg\"/>\n</p>\n\n### Redis schema\n\n<p align=\"center\">\n    <img src=\"./images/leaderboard/leaderboard-schema-redis.svg\"/>\n</p>\n\n1. here Game and player table is having `1 to many` bsc one player can play multiple games\n2. Friends and player is like follower-followee relationship. (associate entity)\n3. Games and leaderboard is `1 to many` as one game can have multiple leaderboards like regional, global, friend_circle.\n4. Players and leaderboard is also `1 to many` as one player can be part of multiple games so multiple leaderboards.\n5. SQL database is good if we have to design system for mid level usage and for high usage system Relational DB is not favourable bcs of few reason\n\n   1. Everytime we have to calculate the rank the full table lookup is needed\n   2. Also if we do indexing the write will become expensive and it will not bring much speed\n   3. Caching will have stale data issue\n   4. the computation of rank in player will require nested queries will not be efficient.\n\n6. For our usecase we need a DB that store data in sorted manner so the fetch of data is very fast. [Redis sorted-set](../Technologies/redis-sorted-set.md) is a good option for this.\n7. So choice is\n\n   1. Redis sorted set , but as we know redis sorted set only store score and member identifier so we can use a combination of `Sorted set + Hash` to store data.\n   2. identifier can contain data like `user:id` .\n   3. In Hset we can store data like\n\n   ```sh\n   HSET user:123 name \"John Doe\" image \"john.jpg\" location \"USA\" email \"john@example.com\"\n   ```\n\n   4. Image can be stored in object storage Amazon S3\n\n8. How we will achieve other functionalities in case of redis DB\n\n   1. leaderboard based on game - make a new sorted set\n   2. Similar can be done for based on location and all\n   3. or another thing can be we can store other data in DB (other than redis) and fetch data based on user id\n\n### High Level Design\n\n<p align=\"center\">\n    <img src=\"./images/leaderboard/leaderboard-hld.svg\"/>\n</p>\n\n1. The client creates a WebSocket connection to the load balancer for real-time communication\n2. The load balancer delegates the client’s request to the closest data center\n3. The server queries the cache server to display the leaderboard\n4. The server queries the relational database on a cache miss and populates the cache server with fetched data\n5. In cache miss we can use `LRU` policy.\n\n### Design Deep dive\n\n#### Get top 10 players\n\n```sh\nZREVRANGE <key> <start> <stop>\ne.g.\nZREVRANGE leaderboard 0 9\n```\n\nAfter this we can get metadata of the players using the command `HMGET` from the Hash.\n\n#### Get rank of a given player\n\n```\n\nZREVRANK <key> <member>\n\nreturns the rank of member in the sorted set stored at key, with the scores ordered from high to low. The rank is 0-based, which means that the member with the highest score has rank 0.\n\ne.g.\nZREVRANK leaderboard \"user:id\"\n```\n\n#### Get surrounding player for a given game\n\n1. Fetch rank of given player\n2. Get surrounding players based on the rank-x to rank+x\n\n```sh\nZREVRANK leaderboard \"user:id\"\n\nres : (integer) 11\n\nZREVRANGE leaderboard 8 14\n```\n\nFOR MORE DESC visit : [Redis Sorted set](../Technologies/redis-sorted-set.md)\n\n#### How to send score notification update\n\n1. DB change can trigger (when score of a player is beaten by another) serverless function call to notify.\n2. We can also use the [Bloom filter](../premier/Uncategorized/Bloom-filters.md) to make sure user receives notifcation only once on rating change.\n3. we can use [Pub-sub](../premier/Scalability-files/Message-Queue.md#types) pattern for this.\n\nPENDING\n\n#### Global leaderboards\n\n1. We can manage multiple sorted sets for distinct gaming scenarios. The global sorted-set can include the agreegated score across all gaming scenarios.\n2. Can use [ZUNIONSTORE](../Technologies/redis-sorted-set.md#union-multiple-set-to-create-a-global-leaderboard) command.\n\n#### Historical leaderboards\n\n<p align=\"center\">\n    <img src=\"./images/leaderboard/historical-leaderboard.svg\"/>\n</p>\n1. We can use ttl based cache for historical leaderboards and also we can utilize REST instead of socket as global leaderboard can use polling, and big in size.\n2. Historical we can move to cols storage for cost efficiency\n3. Flow\n   1. Client checks in CDN for extremely popular leaderboards\n   2. client -> LB -> server -> serverless fn\n   3. serverless function is easy to scale and cost effetive, it checks cache first and if not found in cache then check relational DB, and for image fetch from object storage.\n\n#### Leaderboards based on daily, weekly, monthly\n\nA new sorted set can be created for different time ranges. At the time end we can calculate from existing and do.\n\n#### Leaderboards for friend circle\n\nwe can create a friend leaderboard\n| Feature | Redis Implementation |\n|-----------------------------|----------------------------------------------------------|\n| **Global Leaderboard** | `ZADD global_leaderboard <score> <user_id>` |\n| **Store Friends** | `SADD friends:<user_id> <friend1> <friend2>` |\n| **Create Friend Leaderboard** | `ZADD leaderboard:<user_id> <score> <friend_id>` |\n| **Fetch Friend Leaderboard** | `ZRANGE leaderboard:<user_id> 0 -1 WITHSCORES` |\n| **Update Friend Leaderboard** | `ZADD leaderboard:<user_id> <updated_score> <friend_id>` |\n\n#### High Availability\n\nEnable autoscaling as using serverless functions\n\n#### Low latency\n\nCan deploy server in multiple region and also using cdn for some usecases. Although sorted set gives logarithmic time complexity.\n\n#### Scalability\n\nCaching layer is added for read-heavy loads as clients viewing leaderboard. we can also partition the data.\n\n#### Reliability\n\nWe can setup monitoring with help of Prometheus as time series data and Grafana as dashboard.\nWe can also implement circuit breaker for fault tolerance.\n\n#### Security\n\n1. JWT for auth\n2. rate limiting\n3. encrypt traffic\n"
  },
  {
    "file": "Questions\\pastebin.md",
    "content": "# Pastebin\n\n## Requirements\n\n### Functional\n\n1. User should be able to paste data upto 1MB size\n2. Customized urls\n3. Expiration (Optional to ask to user or a preset defined)\n4. Support text based data only\n5. User is able to set visibility of a url to public or private\n6. User can delete the paste as well if created by him only\n\n### Non functional\n\n1. High availability\n2. Low latency\n3. Durability\n\n## Estimation\n\n1. Per day write - 1M\n2. read/write ratio - 10:1\n3. Avg paste size - 100 KB\n4. Max paste size - 1 MB\n\n### Traffic\n\n| Desc           | value                                 |\n| -------------- | ------------------------------------- |\n| QPS(write)     | (100k)/(sec in one day) = 10 (approx) |\n| QPS(read)      | 10\\*10 = 100                          |\n| Peak QPS(read) | 40 (assumption)                       |\n\n### Storage\n\n| Desc               | value                   |\n| ------------------ | ----------------------- |\n| storage per day    | 1MBx1M = 1 TB           |\n| storage for 5 year | 1 GB x 365 x 5 = 1.8 PB |\n\n### Bandwidth\n\n| Desc    | value                                                  |\n| ------- | ------------------------------------------------------ |\n| Ingress | 1MB/paste x1M paste/day x 10^(-5)day/sec = 10 MB/sec   |\n| Egress  | 1MB/paste x10M reads/day x 10^(-5)day/sec = 100 MB/sec |\n\n### Memory(Caching)\n\nWe can follow the 80/20 rule while caching where 80% of the trafic is server by 20% cached result and rest can be done from server. We can use cache with a TTL of 1 day.\n\n| Desc       | value                                 |\n| ---------- | ------------------------------------- |\n| cache size | 1MB/paste x10M paste/day x 0.2 = 2 TB |\n\n## API Design\n\nWe can use REST for ease of loose coupling and easiness to debug.\n\n### Create paste\n\nRequest\n\n```sh\n/pastes\nmethod: POST\nauthorization:...\n{\n   name:<string>,\n   content:<string>,\n   visibility:<enum>,\n   custom:<string>, (optional)\n   expiry:<dattime> (optional)\n}\n```\n\nResponse\n\n```sh\n201\n{paste-id-url}\n\n401 - unauth\n```\n\n### Get paste\n\nRequest\n\n```sh\n/pastes/:paste-id\nmethod: GET\nauthorization:...\n```\n\nResponse\n\n```sh\n200\n{\n   name:<string>,\n   content:<string>,\n   visibility:<enum>,\n   id:<string>, (optional)\n   expiry:<dattime> (optional),\n   s3_link:<string>\n}\n\n\n401 - unauth\n```\n\n### Delete paste\n\n```sh\n/pastes/:paste-id\nmethod: DELETE\nauthorization:...\n```\n\n## Database\n\n1. As there it is going to be a lot of data so we need to make decision which DB we want to use. I am planning to choose SQL because of this point\n   1. Strict schema\n   2. relational data\n   3. need of complex joins\n   4. lookup by index\n2. Also there is size very huge so we can store huge content on s3 object storage as it will cost effectively and reduce DB io.\n3. Some alternative to S3 is mongodb.\n\n<p align=\"center\">\n   <img src=\"./images/pastebin/schema-design.svg\"/>\n</p>\n\n## HLD\n\n### Encoding\n\n1. we need to encode our pasteId into some format for readability. we can use base58 format.\n2. base58 is similar to base62 bur it doesn't contain non-distinguishable char like (`O (uppercase O), 0 (zero), and I (capital I), l (lowercase L)`).\n3. so range of base58 - A-z a-z 0-9 (exclude above four char)\n4. Total paste id possible for a 8 length base58 id = 58^8 = **128 trillion**\n\n### Write paste\n\n<p align=\"center\">\n   <img src=\"./images/pastebin/write.svg\"/>\n</p>\n\n1. Single machine solution will not scaleout so we move the Key Generation service (**KGS**) outside as a service.\n2. Operations done when client enters a paste\n   1. write call is rate limited\n   2. KGS creates a unique encoded pasteid\n   3. we get [pre-signed url](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html)\n      1. system requests a pres-signed url from the object storage.\n      2. A presigned URL allows the client to upload content directly to storage without needing authentication each time.\n   4. Paste url is created by appending pasteid.\n      1. `http://presigned-url/paste-id`\n   5. The paste content is transferred directly from the client to the object storage using the paste URL to optimize bandwidth expenses and performance\n   6. The object storage persists the paste using the paste URL\n   7. The metadata of the paste including the paste URL is persisted in the SQL database\n   8. The server returns the paste ID to the client for future access\n3. Some other things we can perform on server\n   1. Huffman encoding for reducing text size\n   2. content encryption\n   3. Use bloom filter in case if custom url is requested, we can check if custom url is already present or not\n4. **Random ID Generation**\n   1. we can pick one of the following multiple approach\n      1. [Twitter's snowflake](./snowflake.md)\n      2. MD5 + hashing (something that we followd in url shortener)\n\n### Read paste\n\n<p align=\"center\">\n   <img src=\"./images/pastebin/read.svg\"/>\n</p>\n\n1. Using cache-aside pattern and for eviction use LRU\n2. We are introducing cache at following level\n   1. CDN (public cache) - reduce load on system\n   2. internal cache on data store\n   3. client side cache (browser)\n3. API gateway handles\n   1. rate limiting\n   2. Auth\n   3. compressing\n   4. filtering\n4. Bloom filter is used to avoid cache thrashing, we set bloom fiters when an element accessed more than twice, and if bloom found for paste then only it will be written to cache.\n5. We will shard the DB based on user_id\n\n## Deep dive\n\n### Scalability\n\n1. Scalaing a system is a iterative process we can continuously perform these action\n   1. benchmark or load test\n   2. profiling for bottlenecks and SPOF\n2. We can keep the service stateless for horizontal scaling\n\n### Rate limiting\n\n1. to prevent malicious attacks, we can use following entities to identify\n   1. token for loggedin user\n   2. request from free and premius clients are throttled diff based on the plan\n\n### Availability\n\nCan be approved by following things\n\n1. LB run in active-active or active-passive mode\n2. same like KGS also run in both of above mentioned\n3. Backup DB atleast once a day\n\n### Fault Tolerance\n\n1. we are already using MS arch so it will improve fault tolerance\n2. we can durther can implement circuit breaker and backpressure\n\n### Analytics\n\n1. setup a Kafka for analytics purpose and process event asynchronously\n\n### DB cleanup\n\nWe can remove expired data to reduce data cost by\n\n1. lazy removal\n2. dedicated cleanup service\n3. timer cleanup service\n"
  },
  {
    "file": "Questions\\snowflake.md",
    "content": "# Distributed Unique Id genrator\n\n## Applications\n\nIn most system where Requirements is to get unique id in a distributed env like `Url Shortener`, `Bank`, `MySQL or any other DB`.\n\n## Requirements\n\n### Functional\n\n1. each time a service is called it should return a unique ID across all node in the system\n2. Id should be fixed size, let's say 64 bit\n3. Id should contain a creation timestamp\n\n### Non-Functional\n\n1. Highly consistent, means no duplicate key generation\n2. Able to handle hight throughput\n\n## Solution\n\n### Single Machine(node)\n\nFor single machine we can use multiple approach like\n\n1. Auto increment id\n2. Auto increment id with creation timestamp of the system\n\n### Random Numbers(Multi node)\n\nThis appraoch can work whenw we have to select ids from long range.\n\n#### Universally Unique Identifier(UUID)\n\n`550e8400-e29b-11d4-a716-446655440000`\n\n1. This a uniqueid generated from a set of 2^128 ids available, It is 128 bits.\n2. Based on the version of it the generation of id is diff\n   1. **V1**\n      1. use current timestamp and device's MAC address\n   2. **V3**\n      1. use MD5 hash logic\n   3. **V4**\n      1. use total random numbers\n      2. most used\n   4. **V5**\n      1. uses SHA-1\n3. **Collision Probability**\n   1. Due to v4 version ensure 122 bits reserved for unique numbers so prob of collision is very low\n   2. After generating trillions of urls the prob is still less than 0.5\n4. **Advantages**\n   1. collision is very rare\n   2. non predictable as use random numbers (specially used in sensitive transactions)\n5. **Disadv**\n   1. as uses 128-bit so take extra storage space\n   2. Can not use sorting\n\n#### Twitter snowflake\n\n<p align=\"center\">\n    <img src=\"./images/id-generater/snowflake.webp\"/>\n</p>\n\n1. It is one of the most popular algorithms to generate unique ids in a distributed system.\n2. It is in half of the size of the UUID.\n\n3. Some properties of the Snowflake id is\n   1. Ids are unique and sortable based on timestamp\n   2. Ids include timestamp\n   3. is a 64-bit\n   4. only numerical values\n4. **Structure**\n   | Bits | Field | Description |\n   |------|-------------|--------------------------------------------------|\n   | 1 | Sign bit | Always `0` (ensures positive number). |\n   | 41 | Timestamp | Milliseconds since a custom epoch (ensures order). |\n   | 10 | Machine ID | Identifies the machine or data center (prevents conflict). |\n   | 12 | Sequence | Increments per millisecond to prevent collisions (0-4095). |\n5. **Breakdown**\n   1. **Timestamp (41 bits):** Allows for 69 years of usage (2^41 milliseconds).\n   2. **Machine ID (10 bits):** Supports up to 1024 machines generating IDs.\n   3. **Sequence (12 bits):** Can generate 4096 unique IDs per millisecond per machine.\n"
  },
  {
    "file": "Questions\\steps-to-answer-sd.md",
    "content": "# Steps to answer system design questions\n\n<p align=\"center\">\n    <img src=\"./images/steps.jpg\"/>\n</p>\n\n## Requirement Gathering\n\n1. **Functional**\n   1. Core features\n   2. Critical features\n   3. Users of system (internal, cust, etc.)\n   4. How will user interact (API, web, mobile...)\n   5. What actions user allowed to perform\n2. **Non-Functional**\n   1. Scale of system in term of users and requests\n   2. Data volume we expect\n   3. read to write ratio\n   4. Availability of how many 9's\n\n## Capacity estimation\n\nIt wont be needed in depth in each so ask intervieewer beforehand.\n\n1. Daily users count we can expect\n2. Traffic (peak read/write scenarios)\n3. Storage (Type of data to be stored) and estimation of total amount of storage required\n4. Memory (will caching we helpful)\n\n## High Level design\n\n1. break down into smaller pieces like LB, FE, BE, Cache, DB, Queues etc.\n\n## DB design\n\n1. Choose between SQL vs NOsql based on Data modeling needed.\n2. Design DB schema\n3. Define Data access patterns, and add index and caching based on that\n\n## API Design\n\n1. Indentify the API requirements, based on the functionalities,\n2. Choose api style (REST, Graphql, gRPC) based on client need\n3. Define Endpoints\n4. Specify data formats (JSON, xml)\n\n## Deep dive into Key components\n\n1. Deep dive into high level of business logic like each service , what will it do\n2. Also explain LB, caching, Authentication etc.\n\n## Address Key issues\n\n1. Address Scalability, performance concerns\n2. Reliability\n"
  },
  {
    "file": "Questions\\TinyUrl.md",
    "content": "# TinyUrl\n\n## High level design\n\n<p align=\"center\">\n   <image src='./images/tinyurl-hld.png'/>\n</p>\n\n## Estimation\n\n<p align=\"center\">\n   <image src='../diagrams/tinyurl.drawio.svg'/>\n</p>\n\n## Requirement Gathering\n\n1. Functional\n   1. Generate long to short url\n   2. Redirection\n   3. Support link expiration\n   4. Clicks monitoring (Optional)\n   5. Allow aliasing (Optional)\n2. Non Functional\n   1. High availability (99.9%)\n   2. Scalability\n\n## Deep dive into components\n\n### URL generation service\n\n1. It will handle generation of unique URLs.\n2. Handle collisions.\n3. **Approach 1 for URL generation : Hashing and Encoding**\n   1. Convert `long_url` into a hash using `MD5` or `SHA-256`\n   2. Encode that hash into URL friendly `Base62`.\n   3. Not take few bytes to represent short url.\n   4. Collision handeling\n      1. Make DB query before hand\n      2. Make short_url as unique and rely on the DB contraint to fail and on failure append some increment suffix(like `-1`,`-2`).\n4. **Approach 1 for URL generation : Incremental IDs**\n   1. instead of hashing we can rely on Database incremental ids.\n   2. Convert id into url friendly using base62.\n   3. Consideration\n      1. DB dependency can be bottleneck in scaling\n      2. Harder for distributed systems to implement\n5. **Custom Alias**\n   1. This we can allow based on whether same alias is present in DB or not.\n   2. Validate character in alias to be URL frendly\n   3. if alias already taken return correct errors\n6. **Link expiration**\n   1. Can be done using running a `cron job`, which removed expired links from DB\n   2. During the redirection process service can check if link is expired return to a default page and show error.\n\n### Redirection service\n\n1. DB lookup and then redirection\n2. send HTTP redirect response (301)\n3. For performance can cache in-memory.\n\n### Analytics Service\n\n1. Use a message Queue like Kafka to track each event click.\n2. Batch process agreegation of data.\n\n## Key Issues and Bottlenecks\n\n### Scalability\n\n1. Deploy `API layer` across multiple instances behind load-balancer to distrivute incoming requests evenly.\n2. **Sharding**\n   1. If we are using auto increment id then we can shard based on range.(0-10k,...)\n   2. Other case we can use Hash based sharding. When adding more shards we may need to use consistent hashing to handle.\n3. **Caching**\n   1. store frequently accessed urls.\n\n### Availability\n\n1. DB replications (Single master)\n2. Geo distributed Deploy\n\n### Security\n\n1. Rate limiting\n2. input validation\n3. HTTPS\n"
  },
  {
    "file": "README.md",
    "content": "Notes for System Design Learning (In Progress)\r\n\r\n# HLD\r\n\r\n### [Scalability](premier/Scalability.md)\r\n\r\n- [Caches](premier/Scalability-files/Caches.md)\r\n- [Clone](premier/Scalability-files/Clone.md)\r\n- [Database Scaling](premier/Scalability-files/Database-Scaling.md)\r\n\r\n### [Availability and Consistency](premier/Availability-Consistency.md)\r\n\r\n- [Availability](premier/Availability-Consistency.md#availability)\r\n- [CAP Theorem](premier/Availability-Consistency.md#cap-theorem)\r\n- [PACELC Theorem](premier/Availability-Consistency.md#pacelc-theorem)\r\n\r\n### [DNS, CDN, Load balancer, Proxies](premier/DNS-CDN-Load_balancer-Proxies.md)\r\n\r\n- [Domain Name System](premier/DNS-CDN-Load_balancer-Proxies.md#domain-name-system)\r\n- [CDN](premier/DNS-CDN-Load_balancer-Proxies.md#cdn)\r\n- [Load Balancer](premier/DNS-CDN-Load_balancer-Proxies.md#load-balancer)\r\n- [Proxy Servers](premier/DNS-CDN-Load_balancer-Proxies.md#proxy-servers)\r\n- [Load Balancer vs Reverse Proxy](premier/DNS-CDN-Load_balancer-Proxies.md#load-balancer-vs-reverse-proxy)\r\n\r\n### [Databases](premier/Databases.md)\r\n\r\n- [Relational Database](premier/Databases.md#relational-database)\r\n- [Non-Relational Database (NoSql)](premier/Databases.md#non-relational-database-nosql)\r\n- [SQL vs NoSql](premier/Databases.md#sql-vs-nosql)\r\n\r\n### [Asynchronism](premier/Asynchronism.md)\r\n\r\n- [Message Queues](premier/Scalability-files/Message-Queue.md)\r\n- [Task Queues](premier/Scalability-files/Task-Queue.md)\r\n- [Back Pressure](premier/Asynchronism.md#back-pressure)\r\n\r\n### [Things to Know when building Microservice](premier/Things-to-Know-when-building-Microservice.md)\r\n\r\n- [Service Discovery](premier/Things-to-Know-when-building-Microservice.md#service-discovery)\r\n\r\n### [Communication](premier/Communication.md)\r\n\r\n- [HTTP](premier/Communication.md#http)\r\n- [HTTP1.1 vs HTTP2 vs HTTP3](premier/Communication.md#http11-vs-http2-vs-http3)\r\n- [Transmission Control Protocol (TCP)](premier/Communication.md#transmission-control-protocol-tcp)\r\n- [User Datagram Protocol (UDP)](premier/Communication.md#user-datagram-protocol-udp)\r\n- [Remote Procedure Call](premier/Communication.md#remote-procedure-call-rpc)\r\n- [Representational State Transfer](premier/Communication.md#representational-state-transfer-rest)\r\n- [gRPC](premier/Communication.md#google-remote-procedure-callgrpc)\r\n- [GraphQL](premier/Communication.md#graphql)\r\n- [Websockets](premier/Communication.md#websockets)\r\n- [Server sent Events (SSE)](premier/Communication.md#server-sent-events-sse)\r\n- [Websockets vs SSE vs Long polling vs Polling](premier/Communication.md#websocket-vs-sse-vs-long-polling-vs-polling)\r\n\r\n### [High level Tradeoffs](premier/High-level-tradeoffs.md)\r\n\r\n- [Performance vs Scalability](premier/High-level-tradeoffs.md#performance-vs-scalability)\r\n- [Latency vs Throughput](premier/High-level-tradeoffs.md#latency-vs-throughput)\r\n- [Availability vs Consistency](premier/High-level-tradeoffs.md#availability-vs-consistency---see-in-page-availability-and-consistency)\r\n- [Batch vs Stream Processing](premier/High-level-tradeoffs.md#batch-vs-stream-processing)\r\n- [Stateful vs Stateless](premier/High-level-tradeoffs.md#stateful-vs-stateless-design)\r\n- [Concurrency vs Parallelism](premier/High-level-tradeoffs.md#concurrency-vs-parallelism)\r\n\r\n### Uncategorized\r\n\r\n- [Heartbeat](premier/Uncategorized/Heartbeat.md)\r\n- [Bloom Filters](premier/Uncategorized/Bloom-filters.md)\r\n  - [Traditional vs Scalable vs Counting](/premier/Uncategorized/Bloom-filters.md#traditional-vs-counting-vs-scalable-bloom-filter)\r\n- [Client server Architecture](premier/Uncategorized/client-server-architecture.md)\r\n- [Serverless Architecture](premier/Uncategorized/serverless-arch.md)\r\n- [Cache Control in HTTP](premier/Uncategorized/browser-cache-control.md)\r\n- [Gossip protocol](premier/Uncategorized/gossip-protocol.md)\r\n- [Consensus in Distributed System](premier/Uncategorized/consensus-in-ds.md)\r\n- [Idempotency in Distributed System](premier/Uncategorized/Idempotency.md)\r\n- [Normal and Distributed Locks](premier/Uncategorized/distributed-locks.md)\r\n  - [Normal locks](premier/Uncategorized/distributed-locks.md#normal-locking-single-node-locking)\r\n  - [Distributed locks](premier/Uncategorized/distributed-locks.md#distributed-locking-multi-node-locking)\r\n\r\n### [Types of Attacks](premier/attacks.md)\r\n\r\n- [Denial of Service (DoS)](premier/attacks.md#denial-of-a-service-dos)\r\n- [Distributed Denial of Service (DoS)](premier/attacks.md#distributed-denial-of-a-service-ddos)\r\n- [Man In Middle](premier/attacks.md#main-in-the-middle-mitm)\r\n- [Dns Spoofing](premier/attacks.md#dns-spoofing)\r\n- [Sql Injection](premier/attacks.md#sql-injection)\r\n- [Cross Site Scripting(XSS)](premier/attacks.md#cross-site-scripting-xss)\r\n- [Cross Site Request Forgery(CSRF)](premier/attacks.md#cross-site-request-forgery-csrf)\r\n- [Remote Code Execution(RCE)](premier/attacks.md#remote-code-execution)\r\n- [Path Traversal Attacks](premier/attacks.md#path-traversal-attack)\r\n- [Brute force attacks](premier/attacks.md#brute-force-attack)\r\n- [Session Hijacking](premier/attacks.md#session-hijacking)\r\n- [OAuth and JWT Attacks](premier/attacks.md#oauth-and-jwt-attacks)\r\n\r\n# LLD\r\n\r\nPending\r\n\r\n- OOps\r\n- SOLID\r\n- DRY\r\n- KISS\r\n- YAGNI\r\n\r\n# Technologies\r\n\r\n- [Redis sorted set](Technologies/redis-sorted-set.md)\r\n\r\n# Questions (HLD)\r\n\r\n- [How to answer](Questions/steps-to-answer-sd.md)\r\n- [TinyUrl](Questions/TinyUrl.md)\r\n- [Leaderboard](Questions/leader-board.md)\r\n- [Pastebin](Questions/pastebin.md)\r\n- [Distributed Unique Id generator based on snowflake](Questions/snowflake.md)\r\n- [CDN](Questions/cdn.md) - In progress\r\n\r\n# Implementations\r\n\r\n- [Learderboard in Redis](https://github.com/Ayush-k-Shukla/leaderboard-redis)\r\n- [Id Generation using Snowflake](https://github.com/Ayush-k-Shukla/small-dev-projects/tree/main/3.%20snowflake-id)\r\n- [Server Sent Events](https://github.com/Ayush-k-Shukla/small-dev-projects/tree/main/5.%20server-sent-events)\r\n\r\n# Pending\r\n\r\nTheory\r\n\r\n1. How do DB index work (e.g. Ds used and implemented like B+...)\r\n2. Web app firewall\r\n3. Learn more about OSI model and what work in which layer\r\n4. Back of envelope estimation in sd\r\n5. What happens when we type in url browser\r\n\r\nQuestions\r\n\r\n1. CDN design\r\n2. UPI\r\n\r\nImpl\r\n\r\n1. something related to mq\r\n"
  },
  {
    "file": "Technologies\\redis-sorted-set.md",
    "content": "# Redis Sorted Sets\n\n## 1. Why It Is Used\n\n1. Redis sorted sets are a data structure that store unique elements, each associated with a floating-point score. The elements are automatically sorted based on their scores, making sorted sets useful for scenarios requiring ordered data retrieval and efficient range queries.\n2. If two members have the same score, Redis sorts them based on members’ lexicographical order.\n\n## 2. Use Cases\n\nSorted sets are commonly used for:\n\n- **Leaderboards**: Maintaining dynamically updated rankings (e.g., gaming scores, competition rankings).\n- **Rate Limiting**: Implementing sliding-window rate limiting to control API request rates.\n- **Priority Queues**: Managing tasks based on priority.\n- **Ranking Systems**: Keeping track of top performers or trending items in real-time applications.\n\n## 3. Practice Examples\n\n### Adding Elements to a Sorted Set\n\n```sh\nZADD <name_of_sorted_set> <score> <member>\n```\n\n```sh\nZADD leaderboard 100 \"Alice\"\nZADD leaderboard 200 \"Bob\"\n```\n\n### Change value of score for a member\n\n```sh\nZINCRBY <name_of_sorted_set> <increment> <member>\n```\n\n```sh\nZINCRBY players 10 \"Alice\"\nZINCRBY players -10 \"Bob\"\n```\n\n### Retrieving the Top Players\n\nreturns members in reversed-rank order, with scores ordered from high to low. You have to specify starting and ending rank index positions with a start and stop parameters.\n\n```sh\nZREVRANGE <key> <start> <stop> [WITHSCORES]\n```\n\n```sh\n// for top 10 players\nZREVRANGE leaderboard 0 9 WITHSCORES\n```\n\n### Retrieve the rank and the score of an individual player\n\n```sh\n[ZRANK|ZREVRANK] <key> <member>\n```\n\n### Removing a Player\n\n```sh\nZREM <key> <member> [<member> …]\n```\n\n```sh\nZREM leaderboard \"Alice\"\n```\n\n### Union multiple set to create a global leaderboard\n\n```sh\nZUNIONSTORE destination numkeys key [key ...] [WEIGHTS weight\n  [weight ...]] [AGGREGATE <SUM | MIN | MAX>]\n```\n\nIt makes the union of multiple sorted sets passed as `key` and stores the result in location `destination`.\nUsing WEIGHT option is to specify a multiplication factor for each set score before union, by default it is 1.\nAgreegate option is used to specify what we need from union, sum, max, min.\nIf destination already exist will be overwritten\n\n```sh\nZUNIONSTORE global_leaderboard 2 leader_board_game1 leader_board_game2 WEIGHTS 2 3\n```\n\n## 4. Time Complexity\n\n- **ZADD**: O(log N) (Adding an element)\n- **ZRANGE**: O(log N + M) (Retrieving elements in a range, where M is the number of results)\n- **ZRANGEBYSCORE**: O(log N + M)\n- **ZREM**: O(log N) (Removing an element)\n- **ZREVRANK**: O(log N) (Getting reverse rank of an element)\n- **ZUNIONSTORE**: O(N) + O(M log M) (with N being the sum of the sizes of the input sorted sets, and M being the number of elements in the resulting sorted set.)\n\nRedis sorted sets efficiently handle ordered data, making them ideal for real-time applications requiring fast insertions, deletions, and queries.\n"
  }
]